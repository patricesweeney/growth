{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccaadb8f-28d9-454b-a4b9-ae0d1c93845d",
   "metadata": {},
   "source": [
    "# MaxEnt Mixed Logit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657d7889-388f-440c-aa26-2f89c8be1b6a",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db99224a-a2b5-4e05-b879-f70c70e2d019",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Model specification**\n",
    "\n",
    "$$\n",
    "P(y_{it} = j | X_i, \\alpha_{ji} \\boldsymbol{\\beta_{i}}) = \\frac{e^{\\alpha_{ji} + \\boldsymbol{\\beta}_k' \\boldsymbol{x}_{jit}}}{\\sum_{j \\in C} e^{\\alpha_{ji} + \\boldsymbol{\\beta}_i' \\boldsymbol{x}_{jit}}}\n",
    "$$\n",
    "\n",
    "\n",
    "- $P(y_{it} = j | X_i, \\alpha_{ji} \\boldsymbol{\\beta_{i}})$: Probability of individual $i$ choosing a particular alternative $j$ at time $t$, conditioned on parameters $\\boldsymbol{\\beta_i}$ and $\\alpha_{ji}$\n",
    "- $y_{it}$: Choice outcome for individual $i$ at time $t$.\n",
    "- $x_{jit}$: Set of attributes of choice $j$ for individual $i$ at time $t$.\n",
    "- $\\alpha_{ji}$: Intercept term for choice $j$ for individual $i$, assumed to be a random variable varying across choices.\n",
    "- $\\beta_k$: Coefficient vector attributes $k$, assumed to be a random variable varying across individuals.\n",
    "- $C$: Set of available choices.\n",
    "\n",
    "**Model parameters**\n",
    "\n",
    "In Bayesian terms, the problem is to infer the joint posterior distribution of the model's unknown parameters.\n",
    "\n",
    "$$P(\\theta | \\mathcal{D}) = \\dfrac{P(\\mathcal{D} | \\theta) P(\\theta)}{P(\\mathcal{D})}$$\n",
    "\n",
    "where $\\theta =  \\{ \\alpha_{ji}, \\boldsymbol{\\beta}_i \\}$ and $\\mathcal{D} = \\{ x_{jit}, y_{it} \\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af2e6c1",
   "metadata": {},
   "source": [
    "### Model complexity\n",
    "\n",
    "\n",
    "The model can be more or less complex, depending on how flexible $\\alpha$ and $\\beta$ are.\n",
    "\n",
    "**Multinomial logit models**\n",
    "\n",
    "The key feature of MNL models is that $\\beta_k$ does not vary over the customers $i$, making it a standard deterministic vector.\n",
    "\n",
    "$$\n",
    "\\beta = \\begin{bmatrix}\n",
    "\\beta_0  \\\\\n",
    "\\beta_1 \\\\\n",
    "\\vdots \\\\\n",
    "\\beta_k\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- In the least complex case,  $\\alpha$ and $\\beta$ are fixed for all customers, for all times, for all choices. This reduces to the multinomial logit model. 1 + $k$ parameters. \n",
    "- Getting more complex, $\\alpha$ can vary for each choice, while $\\beta$ remains fixed for all customers, for all times, for all choices.  $j + k$ parameters.\n",
    "- Getting more complex, $\\alpha$ can vary for each choice, while $\\beta$ remains fixed for all customers, for all times, but varies over the choices.  $j + kj$ parameters.\n",
    "\n",
    "\n",
    "**Mixed logit models**\n",
    "\n",
    "The key feature of MNL models is that $\\beta$ does vary over the customers, making it a random vector. We denote this vector $\\phi$ to distinguish it from the deterministic case.\n",
    "\n",
    "$$\n",
    "\\phi = \\begin{bmatrix}\n",
    "\\beta_0  \\sim \\mathcal{N}(\\mu, \\sigma) \\\\ \n",
    "\\beta_1 \\sim \\mathcal{N}(\\mu, \\sigma) \\\\\n",
    "\\vdots  \\\\\n",
    "\\beta_k \\sim \\mathcal{N}(\\mu, \\sigma)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "- In the least complex case,  $\\alpha$ can vary for each choice, while $\\beta$ can vary for each customer, but for each customer stays the same all times, for all choices $j + ki$ parameters.\n",
    "- Getting more complex,  $\\alpha$ can vary for each choice and customer, while $\\beta$ can vary for each customer, but for each customer stays the same all times, for all choices. $ji + ki$ parameters.\n",
    "- Getting more complex,  $\\alpha$ can vary for each choice and customer, while $\\beta$ can vary for each customer, but for each customer stays the same all times, while varying over each choice. $ji + kij$ parameters.\n",
    "- Most complex,  $\\alpha$ can vary for each choice and customer, while $\\beta$ can vary for each customer, at each time, over each choice. $ji + kijt$ parameters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb81c52",
   "metadata": {},
   "source": [
    "### Stochastic $\\beta$ in mixed logit\n",
    "\n",
    "For an individual customer, their $\\beta$ becomes the mean $\\beta_k$ plus a random draw from $\\sigma_k$. This random draw can also be conceptualised as a fixed $\\sigma_k$ scaled by some individual scaling factor $v_i$. For a given attribute $k$, this looks like\n",
    "\n",
    "$$ \\beta_i = \\beta + \\sigma v_i$$\n",
    "\n",
    "\n",
    "Returning to $\\phi$ to take into account all of the attributes $k$, because it is a random vector, it has multivariate normal joint probability distribution.\n",
    "\n",
    "$$\\phi_i = \\phi + \\Gamma v_i$$\n",
    "\n",
    "where $\\Gamma$ is the standard deviation matrix with the sigmas on the main diagonal.\n",
    "\n",
    "$$\n",
    "\\Gamma = \\begin{bmatrix}\n",
    "\\sigma_1 & 0 & \\cdots & 0 \\\\\n",
    "0 & \\sigma_2 & \\cdots & 0 \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "0 & 0 & \\cdots & \\sigma_k\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Correlated attributes**\n",
    "\n",
    "With correlation between different attributes, the covariance matrix becomes\n",
    "\n",
    "$$ \\Sigma = \\Gamma \\Gamma' $$\n",
    "\n",
    "$$\n",
    "\\Sigma = \\begin{bmatrix}\n",
    "\\sigma_1^2 & \\text{Cov}(1, 2) & \\cdots & \\text{Cov}(1, k) \\\\\n",
    "\\text{Cov}(2, 1) & \\sigma_2^2 & \\cdots & \\text{Cov}(2, k) \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\text{Cov}(k, 1) & \\text{Cov}(k, 2) & \\cdots & \\sigma_k^2\n",
    "\\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d112e7d-e3e4-4330-9189-02c5d257c5b1",
   "metadata": {},
   "source": [
    "# Maximum entropy setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1616c30-9b0a-4e19-b279-039aa8231b42",
   "metadata": {},
   "source": [
    "Our ground truth choice data $y_{ij}$ is an $i \\times j$ matrix with individual observations $i$ on the rows and choice outcomes $j$ on the columns.\n",
    "\n",
    "$$\n",
    "Y = \\begin{bmatrix}\n",
    "y_{11} & y_{12} & \\cdots & y_{1j} \\\\\n",
    "y_{21} & y_{22} & \\cdots & y_{2j} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "y_{i1} & y_{i2} & \\cdots & y_{ij}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This makes $y_{ij}$ some link function of observed attributes $x_{ij}$ and $\\beta_{ij}$, plus an error term.\n",
    "\n",
    "$$ y_{ij} = F(x_{ji}' \\beta_j) + \\epsilon_{ij}$$\n",
    "\n",
    "$F(x_{ji}' \\beta_j)$ is just the predicted probability, so \n",
    "\n",
    "$$ y_{ij} = p_{ij} + \\epsilon_{ij}$$\n",
    "\n",
    "In other words, the ground truth choice is the predicted probability plus some error.\n",
    "\n",
    "Because $0 < p_{ij} < 1$ and $y_{ij}$ is either $0$ or $1$, the error term $\\epsilon_{ij}$ is bounded between $[-1,1]$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3de2d5",
   "metadata": {},
   "source": [
    "### Revenue constraint\n",
    "\n",
    "Assuming price is our only attribute (i.e. $k$ = 1), revenue is price ($x$) multiplied by volume ($y$).\n",
    "\n",
    "$$\n",
    "R = \\sum_{ij} y_{ij} \\times x_{ij}\n",
    "$$\n",
    "\n",
    "$$\n",
    "R = \\sum_{ij} \\left( p_{ij} + \\epsilon_{ij} \\right) \\times x_{ij}\n",
    "$$\n",
    "\n",
    "$$\n",
    "R = \\sum_{ij} p_{ij} \\times x_{ij} + \\sum_{ij} \\epsilon_{ij} \\times x_{ij}\n",
    "$$\n",
    "\n",
    "Therefore, total revenue equals predicted revenue plus revenue error.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49dc65d7",
   "metadata": {},
   "source": [
    "\n",
    "### Reformulating $p_{ij}$ and $\\epsilon_{ij}$ as random variables\n",
    "\n",
    "Let $p_{ij}$ be the expected value of a discrete random variable $s_{ij}$, with support $s_m$ $\\in [0,1]$ and underlying probabilities $p(s_{ijm}) = \\pi_{ijm}$. \n",
    "\n",
    "$$ \\langle s_{ij} \\rangle  = p_{ij} $$\n",
    "\n",
    "$$ \\langle s_{ij} \\rangle  = \\sum_m s_m \\pi_{ijm} $$\n",
    "\n",
    "\n",
    "\n",
    "Let $\\epsilon_{ij}$ be expected value of discrete random variable $u_{ij}$, with support $u_h \\in [-1,1]$ and underlying probabilities $p(u_{ijh}) = w_{ijh}$.\n",
    "\n",
    "$$  \\langle u_{ij} \\rangle = \\epsilon_{ij} $$\n",
    "\n",
    "$$ \\langle u_{ij} \\rangle  = \\sum_h u_h w_{ijh} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1bb779",
   "metadata": {},
   "source": [
    "\n",
    "### Reformulated revenue constraint\n",
    "\n",
    "$$\n",
    "R = \\sum_{ij} y_{ij} \\times x_{ij}\n",
    "$$\n",
    "\n",
    "We substitute in our new definitions of $p_{ij}$ and $\\epsilon_{ij}$.\n",
    "\n",
    "$$\n",
    "R = \\sum_{ij} p_{ij} \\times x_{ij} + \\sum_{ij} \\epsilon_{ij} \\times x_{ij}\n",
    "$$\n",
    "\n",
    "Substituting in their expected value representation\n",
    "\n",
    "$$\n",
    "R = \\sum_{ij} \\langle s_{ij} \\rangle \\times x_{ij} + \\sum_{ij} \\langle u_{ij} \\rangle \\times x_{ij}\n",
    "$$\n",
    "\n",
    "Putting the expected values in explicit form\n",
    "\n",
    "$$\n",
    "R = \\sum_{ijm} s_m \\pi_{ijm} \\times x_{ij} + \\sum_{ijh} u_h w_{ijh} \\times x_{ij}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d45f4a0",
   "metadata": {},
   "source": [
    "# Maximum entropy model\n",
    "\n",
    "Our job is to infer the values of $\\pi_{ijm}$ and $w_{ijh}$ by maximising an entropy (objective) function.\n",
    "\n",
    "We assume the model is correctly specified, such that $p$ and $\\epsilon$ (and therefore $\\pi$ and $w$) are independent, and the maximum joint entropy is the sum of the maximum marginal entropies.\n",
    "\n",
    "### Objective function\n",
    "\n",
    "$$\\max_{\\pi, w} H(\\pi, w) =  \\left( - \\sum_{ijm} \\pi_{ijm} \\log \\pi_{ijm} \\right) +  \\left( - \\sum_{ijh} w_{ijh} \\log w_{ijh} \\right) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62a73b3",
   "metadata": {},
   "source": [
    "### Constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360a69c5",
   "metadata": {},
   "source": [
    "**Revenue constraint**\n",
    "$$ R = \\sum_{ij} y_{ij} \\times x_{ij} = \\sum_{ijm} s_m \\pi_{ijm} \\times x_{ij} + \\sum_{ijh} u_h w_{ijh} \\times x_{ij} $$\n",
    "\n",
    "**$\\pi$ and $w$ must be legitimate probability distributions over the values of $p$ and $\\epsilon$**\n",
    "$$\\sum_m \\pi_{ijm} = 1$$\n",
    "\n",
    "$$\\sum_h w_{ijh} = 1$$\n",
    "\n",
    "**$p_{ij}$ must be a legitimate probability distribution over the choices $j$**\n",
    "$$ \\sum_m s_m \\pi_{ijm} = 1$$\n",
    "\n",
    "\n",
    "In principal, we can formulate this as a Lagrangean and solve it analytically, but in practice we will probably use numerical methods.\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = - \\sum_{ijm} \\pi_{ijm} \\log \\pi_{ijm} - \\sum_{ijh} w_{ijh} \\log w_{ijh} + \\lambda \\left( R - \\sum_{ijm} s_m \\pi_{ijm} \\times x_{ij} - \\sum_{ijh} u_h w_{ijh} \\times x_{ij} \\right) + \\sum_{ij} \\lambda_{ij} \\left( 1 - \\sum_m \\pi_{ijm} \\right) + \\sum_{ij} \\lambda_{ij} \\left( 1 - \\sum_h w_{ijh} \\right) + \\sum_{ij} \\lambda_{ij} \\left( 1 - \\sum_m s_m \\pi_{ijm} \\right)\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b393f3",
   "metadata": {},
   "source": [
    "### Marginal effects\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e53c8f6",
   "metadata": {},
   "source": [
    "### Information gain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c11bdd",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Objective function: Negative of the entropy function\n",
    "def objective_function(params):\n",
    "    # params would be a flattened array of all the pi and w values\n",
    "    # Split params back into pi and w and reshape them as needed\n",
    "    # Calculate the negative entropy\n",
    "    # ...\n",
    "    return -entropy\n",
    "\n",
    "# Constraint functions\n",
    "def revenue_constraint(params):\n",
    "    # Define the revenue constraint equation\n",
    "    # ...\n",
    "    return calculated_revenue - R\n",
    "\n",
    "def probability_constraint_pi(params):\n",
    "    # Define the constraint that sum of pi for each i, j should be 1\n",
    "    # ...\n",
    "    return sum_pi - 1\n",
    "\n",
    "def probability_constraint_w(params):\n",
    "    # Define the constraint that sum of w for each i, j should be 1\n",
    "    # ...\n",
    "    return sum_w - 1\n",
    "\n",
    "# Initial guess for the parameters\n",
    "initial_guess = np.ones(num_variables)  # Adjust the size as needed\n",
    "\n",
    "# Constraints setup\n",
    "constraints = [{'type': 'eq', 'fun': revenue_constraint},\n",
    "               {'type': 'eq', 'fun': probability_constraint_pi},\n",
    "               {'type': 'eq', 'fun': probability_constraint_w}]\n",
    "\n",
    "# Optimization\n",
    "result = minimize(objective_function, initial_guess, constraints=constraints)\n",
    "\n",
    "# Check if the solver found a solution\n",
    "if result.success:\n",
    "    optimized_params = result.x\n",
    "    # Reshape and process optimized_params as needed\n",
    "else:\n",
    "    print(\"Optimization failed:\", result.message)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97853e05-00f6-49ed-9b27-93a936dd37ca",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe58ed9e-043a-46a2-9079-d11527082f23",
   "metadata": {},
   "source": [
    "Manzini, P., Mariotti, M., & Ülkü, L. (2019). Stochastic complementarity. The Economic Journal, 129(619), 1343-1363.\n",
    "\n",
    " \n",
    "\n",
    "Yan, Z., Natarajan, K., Teo, C. P., & Cheng, C. (2022). A representative consumer model in data-driven multiproduct pricing optimization. Management Science, 68(8), 5798-5827."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13875d5-21ef-4d46-bfbf-1697021f855f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EconML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
