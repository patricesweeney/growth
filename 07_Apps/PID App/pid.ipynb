{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\mathcal{D} = \\{y, \\bold{X}\\}$\n",
    "where $y$ is the target variable and $X$ are the source variables.\n",
    "\n",
    "We are interested in what makes $y$ tick.\n",
    "\n",
    "If the generating process for $y$ is ergodic, then the total uncertainty in $y$ is captured by the Shannon entropy.\n",
    "\n",
    "### Shannon entropy (discrete)\n",
    "\n",
    "We can conceive of the Shannon entropy as the average surprise of each observation $y_i$, taking into account the rest:\n",
    "\n",
    "$$H(y) = - \\langle \\log_2 p(y) \\rangle $$\n",
    "\n",
    "$$H(y) = - \\sum_y p(y) \\log_2 p(y) $$\n",
    "\n",
    "The Shannon entropy is measured in bits.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Differential entropy (continuous)\n",
    "\n",
    "If $y$ is 'continuous', (i.e. we are willing to interpolate the discrete histogram) then the differential entropy is \n",
    "\n",
    "$$ H(y) = - \\int_{- \\infty}^{\\infty} p(y) \\log p(y) dy $$ \n",
    "\n",
    "The differential entropy\n",
    "- Excludes the point in the integral where y = 0.\n",
    "- Uses natural logs, and produces a measure in nats.\n",
    "- Can be negative\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional entropy\n",
    "\n",
    "Let's say $H(y) = 10$.\n",
    "\n",
    "The conditional entropy, $H(y | \\bold{X}$) measures the remaining uncertainty after accounting for $\\bold{X}$. \n",
    "\n",
    "Let's assume $H(y|\\bold{X}) = 2$. \n",
    "\n",
    "This is ideal, because the conditional entropy is significantly lower than the entropy $\\bold{X}$. This means that a lot of uncertainty was reduced by conditioning.\n",
    "\n",
    "\n",
    "### Mutual information\n",
    "\n",
    "The difference between the entropy and the conditional entropy is the mutual information.\n",
    "\n",
    "$$MI(y;\\bold{X}) = H(y) - H(y|\\bold{X})$$\n",
    "\n",
    "In the above example, we had 10 bits of uncertainty, and after conditioning we only had 2 This means the mutual information 10 - 2 = 8.\n",
    "\n",
    "**Explained information**\n",
    "\n",
    "An important ratio is the ratio of mutual information to total entropy, which I will call explained information.\n",
    "\n",
    "$$E(y|\\bold{X}) = \\dfrac{MI(y;\\bold{X})}{H(y)}$$\n",
    "\n",
    "The \"explained information\" is just the proportion of entropy that is \"explained\" by conditioning. I.e. the mutual information divided by the total entropy.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MI in depth\n",
    "\n",
    "The formula for MI is relatively simple, it's the total entropy minus the entropy that's left after conditioning. It's the information gain.\n",
    "\n",
    "$$MI(y;\\bold{X}) = H(y) - H(y|\\bold{X})$$\n",
    "\n",
    "This is massively complicated by the fact the $\\bold{X}$ is a matrix. This means, for a simple data set,  what we really have is:\n",
    "\n",
    "$$MI(y; x_1, x_2) = H(y) - H(y|x_1, x_2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partial information decomposition\n",
    "\n",
    "$$MI(y; x_1, x_2) = H(y) - H(y|x_1, x_2)$$\n",
    "\n",
    "We figure out that we can decompose this total information gain from covariates into three sources:\n",
    "- Unique information\n",
    "- Synergistic information\n",
    "- Redundant information\n",
    "\n",
    "\n",
    "$$MI(y; x_1, x_2) = \\text{Uni}(x_1) + \\text{Uni}(x_2) + \\text{Syn}(x_1,x_2) - \\text{Red}(x_1,x_2) $$ \n",
    "\n",
    "Or more rigorously\n",
    "\n",
    "$$MI(y; x_1, x_2) = \\text{Uni}(y: x_1 | x_2) + \\text{Uni}(y: x_2 | x_1) + \\text{Syn}(y: (x_1,x_2)) - \\text{Red}(y: (x_1,x_2)) $$ \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EconML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
