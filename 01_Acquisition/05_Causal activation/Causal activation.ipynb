{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76fa7eb6-9007-4b3d-8d01-5efa62912181",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9a77d9-eca4-4deb-af10-5c629b3a5362",
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_data_local():\n",
    "    import pandas as pd\n",
    "    file_path = '/Users/patricksweeney/growth/01_Acquisition/05_Causal activation/F7 days.xlsx'\n",
    "    data = pd.read_excel(file_path)\n",
    "    \n",
    "    columns_to_remove = ['paid_signup', 'converted']  # Replace with your column names\n",
    "    data = data.drop(columns=columns_to_remove, axis=1)\n",
    "\n",
    "    return data\n",
    "\n",
    "data = import_data_local()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d70b6d-31b9-42fd-b8ba-8c4785658d77",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b8a757-6bc9-462b-902b-8a4c20c10f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(data, variables):\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Ensure the variables are in the DataFrame\n",
    "    for var in variables:\n",
    "        if var not in data.columns:\n",
    "            raise ValueError(f\"Variable '{var}' not found in the DataFrame\")\n",
    "\n",
    "    # One-hot encode the specified variables\n",
    "    for var in variables:\n",
    "        # Get one-hot encoding\n",
    "        one_hot = pd.get_dummies(data[var], prefix=var, dtype=bool)\n",
    "        # Convert True/False to 1/0\n",
    "        one_hot = one_hot.astype(int)\n",
    "        # Drop the original column\n",
    "        data = data.drop(var, axis=1)\n",
    "        # Join the encoded DataFrame\n",
    "        data = data.join(one_hot)\n",
    "    \n",
    "    return data\n",
    "\n",
    "data = one_hot_encode(data, ['seniority', 'role'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4e9651-1631-419c-8556-b3db9f487602",
   "metadata": {},
   "source": [
    "# Causal discovery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe891dc9-8f1f-41fa-9670-dcf4efcae522",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4970ec4-8b7c-4038-be28-fbc663e2d219",
   "metadata": {},
   "outputs": [],
   "source": [
    "def causal_priors(reverse):\n",
    "    import numpy as np\n",
    "    import networkx as nx\n",
    "    from io import StringIO\n",
    "\n",
    "    GML_priors = \"\"\"\n",
    "    graph [\n",
    "      directed 1\n",
    "        node [id 0 label \"mrr_converted\"]\n",
    "        node [id 1 label \"project_count_f7d\"]\n",
    "        node [id 2 label \"transcription_count_f7d\"]\n",
    "        node [id 3 label \"highlight_count_f7d\"]\n",
    "        node [id 4 label \"tag_count_f7d\"]\n",
    "        node [id 5 label \"insight_count_f7d\"]\n",
    "        node [id 6 label \"reel_created_count_f7d\"]\n",
    "        node [id 7 label \"invite_count_f7d\"]\n",
    "        node [id 8 label \"shared_object_note_count_f7d\"]\n",
    "        node [id 9 label \"shared_object_insight_count_f7d\"]\n",
    "        node [id 10 label \"note_viewed_user_count_f7d\"]\n",
    "        node [id 11 label \"tag_viewed_user_count_f7d\"]\n",
    "        node [id 12 label \"insight_viewed_user_count_f7d\"]\n",
    "        node [id 13 label \"seniority_Department lead\"]\n",
    "        node [id 14 label \"seniority_Executive\"]\n",
    "        node [id 15 label \"seniority_Individual contributor\"]\n",
    "        node [id 16 label \"seniority_Team lead\"]\n",
    "        node [id 17 label \"role_CUSTOMER_SUCCESS\"]\n",
    "        node [id 18 label \"role_DESIGN\"]\n",
    "        node [id 19 label \"role_ENGINEERING\"]\n",
    "        node [id 20 label \"role_FINANCE\"]\n",
    "        node [id 21 label \"role_LEGAL\"]\n",
    "        node [id 22 label \"role_MANAGEMENT\"]\n",
    "        node [id 23 label \"role_MARKETING\"]\n",
    "        node [id 24 label \"role_OPERATIONS\"]\n",
    "        node [id 25 label \"role_OTHER\"]\n",
    "        node [id 26 label \"role_PRODUCT_MANAGEMENT\"]\n",
    "        node [id 27 label \"role_RESEARCH\"]\n",
    "        node [id 28 label \"role_SALES\"]\n",
    "        node [id 29 label \"role_STUDENT\"]\n",
    "        node [id 30 label \"role_SUPPORT\"]\n",
    "        \n",
    "        edge [source 1 target 2 label \"project_count_f7d → transcription_count_f7d\"]\n",
    "        edge [source 2 target 3 label \"transcription_count_f7d → highlight_count_f7d\"]\n",
    "        edge [source 2 target 4 label \"transcription_count_f7d → tag_count_f7d\"]\n",
    "        edge [source 4 target 3 label \"tag_count_f7d → highlight_count_f7d\"]\n",
    "        edge [source 3 target 6 label \"highlight_count_f7d → reel_created_count_f7d\"]\n",
    "        edge [source 3 target 5 label \"highlight_count_f7d → insight_count_f7d\"]\n",
    "        edge [source 6 target 7 label \"reel_created_count_f7d → invite_count_f7d\"]\n",
    "        edge [source 5 target 7 label \"insight_count_f7d → invite_count_f7d\"]\n",
    "        edge [source 6 target 8 label \"reel_created_count_f7d → shared_object_note_count_f7d\"]\n",
    "        edge [source 5 target 9 label \"insight_count_f7d → shared_object_insight_count_f7d\"]\n",
    "        edge [source 6 target 0 label \"reel_created_count_f7d → mrr_converted\"]\n",
    "        edge [source 7 target 0 label \"invite_count_f7d → mrr_converted\"]\n",
    "        edge [source 8 target 0 label \"shared_object_note_count_f7d → mrr_converted\"]\n",
    "        edge [source 9 target 0 label \"shared_object_insight_count_f7d → mrr_converted\"]\n",
    "        edge [source 4 target 0 label \"tag_count_f7d → mrr_converted\"]\n",
    "        edge [source 3 target 0 label \"highlight_count_f7d → mrr_converted\"]\n",
    "        edge [source 8 target 10 label \"shared_object_note_count_f7d → note_viewed_user_count_f7d\"]\n",
    "        edge [source 9 target 12 label \"shared_object_insight_count → note_viewed_user_count_f7d\"]\n",
    "        \n",
    "        edge [source 14 target 0 label \"seniority_Executive → mrr_converted\"]\n",
    "        edge [source 15 target 0 label \"seniority_Individual contributor → mrr_converted\"]\n",
    "        edge [source 16 target 0 label \"seniority_Team lead → mrr_converted\"]\n",
    "        edge [source 17 target 0 label \"role_CUSTOMER_SUCCESS → mrr_converted\"]\n",
    "        edge [source 18 target 0 label \"role_DESIGN → mrr_converted\"]\n",
    "        edge [source 19 target 0 label \"role_ENGINEERING → mrr_converted\"]\n",
    "        edge [source 20 target 0 label \"role_FINANCE → mrr_converted\"]\n",
    "        edge [source 21 target 0 label \"role_LEGAL → mrr_converted\"]\n",
    "        edge [source 22 target 0 label \"role_MANAGEMENT → mrr_converted\"]\n",
    "        edge [source 23 target 0 label \"role_MARKETING → mrr_converted\"]\n",
    "        edge [source 24 target 0 label \"role_OPERATIONS → mrr_converted\"]\n",
    "        edge [source 25 target 0 label \"role_OTHER → mrr_converted\"]\n",
    "        edge [source 26 target 0 label \"role_PRODUCT_MANAGEMENT → mrr_converted\"]\n",
    "        edge [source 27 target 0 label \"role_RESEARCH → mrr_converted\"]\n",
    "        edge [source 28 target 0 label \"role_SALES → mrr_converted\"]\n",
    "        edge [source 29 target 0 label \"role_STUDENT → mrr_converted\"]\n",
    "        edge [source 30 target 0 label \"role_SUPPORT → mrr_converted\"]\n",
    "      ] \"\"\"\n",
    "      \n",
    "\n",
    "    # Use StringIO to treat string as a file for parsing\n",
    "    GML_buffer = StringIO(GML_priors)\n",
    "    G_priors = nx.parse_gml(GML_buffer, label='label')\n",
    "    \n",
    "    # Initialize a matrix with -1 indicating no prior knowledge\n",
    "    n_features = len(G_priors.nodes())\n",
    "    prior_knowledge = np.full((n_features, n_features), -1)\n",
    "\n",
    "    # Update the matrix with 0s and 1s based on the edges in the graph\n",
    "    for i, node_i in enumerate(G_priors.nodes()):\n",
    "        for j, node_j in enumerate(G_priors.nodes()):\n",
    "            if i != j:\n",
    "                edge_exists = G_priors.has_edge(node_i, node_j) if not reverse else G_priors.has_edge(node_j, node_i)\n",
    "                if edge_exists:\n",
    "                    prior_knowledge[i, j] = 1\n",
    "                else:\n",
    "                    prior_knowledge[i, j] = 0\n",
    "\n",
    "    return G_priors, prior_knowledge\n",
    "\n",
    "# Call the function to get the NetworkX graph\n",
    "G_priors, prior_knowledge = causal_priors(reverse = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82020a70-1a04-4100-b23c-1bee0bef2980",
   "metadata": {},
   "source": [
    "### Structure learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f54f994-088a-4d12-91ab-6eee549d23f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def causal_discovery(data, algorithm, prior_knowledge, transform):\n",
    "    import networkx as nx\n",
    "    import matplotlib.pyplot as plt\n",
    "    from scipy.stats.mstats import winsorize\n",
    "    import numpy as np\n",
    "    from castle.common import GraphDAG\n",
    "    from castle.common.independence_tests import CITest\n",
    "    from castle.common.priori_knowledge import PrioriKnowledge\n",
    "    from scipy import stats\n",
    "    from castle.algorithms import PC, GES, ICALiNGAM, DirectLiNGAM, NotearsNonlinear, GOLEM, GAE, DAG_GNN, RL, ANMNonlinear, GraNDAG, Notears, MCSL, NotearsLowRank, PNL, GraNDAG\n",
    "\n",
    "    # Make a copy of the data to avoid modifying the original DataFrame\n",
    "    data_copy = data.copy()    \n",
    "    \n",
    "    # Select the algorithm based on the 'algorithm' argument\n",
    "    if algorithm == 'PC': #fast\n",
    "        algo = PC(variant='stable', alpha=0.01, ci_test = CITest.hsic_test)\n",
    "    elif algorithm == 'GES': #medium\n",
    "        algo = GES(criterion='bic')\n",
    "    elif algorithm == 'GAE': #very slow\n",
    "        algo = GAE(input_dim = len(data_copy.columns))\n",
    "    elif algorithm == 'ANMNonlinear':\n",
    "        algo = ANMNonlinear() #broken\n",
    "    elif algorithm == 'DirectLiNGAM':\n",
    "        if not prior_knowledge is not None:\n",
    "            algo = DirectLiNGAM() #fast\n",
    "        else:\n",
    "            algo = DirectLiNGAM(prior_knowledge = prior_knowledge) #fast\n",
    "    elif algorithm == 'ICALiNGAM':\n",
    "        algo = ICALiNGAM() #fast\n",
    "    elif algorithm == 'Notears':\n",
    "        algo = Notears() #medium\n",
    "    elif algorithm == 'NotearsLowRank':\n",
    "        algo = NotearsLowRank() #medium\n",
    "    elif algorithm == 'NotearsNonlinear':\n",
    "        algo = NotearsNonlinear()\n",
    "    elif algorithm == 'GOLEM':\n",
    "        algo = GOLEM(num_iter=2e4) #medium\n",
    "    elif algorithm == 'DAG_GNN':\n",
    "        algo = DAG_GNN()\n",
    "    elif algorithm == 'PNL':\n",
    "        algo = PNL(device_type='cpu') #broken\n",
    "    elif algorithm == 'GRAN':\n",
    "        d = {'model_name': 'NonLinGauss', 'nonlinear': 'leaky-relu', 'optimizer': 'sgd', 'norm_prod': 'paths', 'device_type': 'cpu'}\n",
    "        algo = GraNDAG(input_dim = len(data_copy.columns))\n",
    "    elif algorithm == 'RL':\n",
    "        algo = RL(nb_epoch=2000) #Slow\n",
    "    elif algorithm == 'MCSL':\n",
    "        algo = MCSL(model_type='nn',\n",
    "          iter_step=100,\n",
    "          rho_thresh=1e20,\n",
    "          init_rho=1e-5,\n",
    "          rho_multiply=10,\n",
    "          graph_thresh=0.5,\n",
    "          l1_graph_penalty=2e-3) #slow\n",
    "    else:\n",
    "        raise ValueError(\"Invalid algorithm specified\")\n",
    "\n",
    "# Select only numeric columns for transformation\n",
    "    numeric_columns = data_copy.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "    for col in numeric_columns:\n",
    "        if data_copy[col].nunique() > 2:  # Check if column is continuous\n",
    "            if transform == 'log':\n",
    "                data_copy[col] = np.log1p(data_copy[col])  # Log transformation\n",
    "            elif transform == 'loghalf':\n",
    "                data_copy[col] = np.log1p(data_copy[col]) + 0.5  # Log transformation plus 0.5\n",
    "            elif transform == 'boxcox':\n",
    "                # Adding a small constant to avoid issues with zero or negative values\n",
    "                data_copy[col], _ = stats.boxcox(data_copy[col] + 0.01)  # Box-Cox transformation\n",
    "            elif transform == 'winsorize':\n",
    "                data_copy[col] = winsorize(data_copy[col], limits=[0.00, 0.001])  # Winsorizing the data\n",
    "\n",
    "    \n",
    "# Check for near-zero variance in numeric columns only\n",
    "    near_zero_variance_cols = data_copy[numeric_columns].var() <= 1e-8\n",
    "    if near_zero_variance_cols.any():\n",
    "        print(\"Warning: Columns with near-zero variance detected:\", near_zero_variance_cols[near_zero_variance_cols].index.tolist())\n",
    "    \n",
    "    #Transform\n",
    "    if algorithm == 'GAE':\n",
    "        data_copy = data_copy.to_numpy()\n",
    "    \n",
    "    #Learn\n",
    "    algo.learn(data_copy)\n",
    "\n",
    "    # Extract the causal matrix\n",
    "    causal_matrix = algo.causal_matrix\n",
    "    if algorithm == 'DirectLiNGAM':\n",
    "        weighted_causal_matrix = algo.weight_causal_matrix\n",
    "\n",
    "    def print_rounded_matrix(matrix):\n",
    "        # Round the matrix values to one decimal place\n",
    "        rounded_matrix = [[round(value, 1) for value in row] for row in matrix]\n",
    "    \n",
    "        # Print the matrix in a square format\n",
    "        for row in rounded_matrix:\n",
    "            print(\" \".join(f\"{value:5}\" for value in row))\n",
    "\n",
    "\n",
    "    # Extract column names as variable names for labels\n",
    "    variable_names = data.columns\n",
    "\n",
    "    # Create a directed graph\n",
    "    G = nx.DiGraph()\n",
    "    \n",
    "    # Add all variables as nodes to the graph\n",
    "    for variable in variable_names:\n",
    "        G.add_node(variable)\n",
    "    \n",
    "    # Add edges based on the causal matrix without weights\n",
    "    for i, row in enumerate(causal_matrix):\n",
    "        for j, col in enumerate(row):\n",
    "            if col != 0:  # Nonzero entries indicate edges\n",
    "                G.add_edge(variable_names[i], variable_names[j])\n",
    "\n",
    "\n",
    "    # Create an adjacency matrix using networkx\n",
    "    adjacency_matrix = nx.adjacency_matrix(G, nodelist=variable_names).toarray()\n",
    "\n",
    "    # Create a heatmap of the adjacency matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(adjacency_matrix, cmap='Greens', interpolation='nearest', aspect='auto')\n",
    "\n",
    "    # Add labels to the x and y axes\n",
    "    plt.xticks(np.arange(len(variable_names)), variable_names, rotation=90)\n",
    "    plt.yticks(np.arange(len(variable_names)), variable_names)\n",
    "\n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar()\n",
    "    cbar.set_label('Edge Weight')\n",
    "\n",
    "    # Show the heatmap\n",
    "    plt.title(f'Causal DAG ({algorithm} Algorithm): Left Causes Right')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    if algorithm == 'DirectLiNGAM':\n",
    "        print_rounded_matrix(weighted_causal_matrix)\n",
    "        print(causal_matrix)\n",
    "    \n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    from matplotlib.colors import ListedColormap\n",
    "    \n",
    "    # Example list of variable_names\n",
    "    # variable_names = ['var_one', 'var_two', 'var_three', ...]\n",
    "    \n",
    "    # Process variable_names to replace underscores with spaces and convert to sentence case\n",
    "    formatted_variable_names = [name.replace('_', ' ').capitalize() for name in variable_names]\n",
    "    \n",
    "    # Additional heatmap for DirectLiNGAM using weighted causal matrix\n",
    "    if weighted_causal_matrix is not None:\n",
    "        # Define a custom colormap\n",
    "        cmap = plt.cm.Blues\n",
    "        my_cmap = cmap(np.arange(cmap.N))\n",
    "        my_cmap[0, :3] = 1  # Set the color for zero values to white (RGB: 1, 1, 1)\n",
    "        my_cmap = ListedColormap(my_cmap)\n",
    "    \n",
    "        # Create a heatmap of the weighted adjacency matrix\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.imshow(weighted_causal_matrix, cmap=my_cmap, interpolation='none', aspect='auto')\n",
    "    \n",
    "        # Add labels to the x and y axes with formatted names\n",
    "        plt.xticks(np.arange(len(formatted_variable_names)), formatted_variable_names, rotation=90)\n",
    "        plt.yticks(np.arange(len(formatted_variable_names)), formatted_variable_names)\n",
    "    \n",
    "        # Add colorbar\n",
    "        cbar = plt.colorbar()\n",
    "        cbar.set_label('Edge Weight')\n",
    "    \n",
    "        # Show the heatmap\n",
    "        plt.title(f'Weighted Causal DAG ({algorithm} Algorithm): Edge Weights')\n",
    "        plt.show()\n",
    "        \n",
    "    return G\n",
    "\n",
    "# Example usage\n",
    "G = causal_discovery(data, 'DirectLiNGAM', prior_knowledge = prior_knowledge, transform = 'loghalf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e860c5-ad89-44cf-b07b-dc373762546a",
   "metadata": {},
   "source": [
    "### Graph DAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0006c3-bc0e-4bec-843b-faf1e156dbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_dag(G, outcome):\n",
    "    import networkx as nx\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import igraph as ig\n",
    "\n",
    "    # Calculating various centrality measures\n",
    "    in_degree_centrality = nx.in_degree_centrality(G)\n",
    "    out_degree_centrality = nx.out_degree_centrality(G)\n",
    "    closeness_centrality = nx.closeness_centrality(G)\n",
    "    betweenness_centrality = nx.betweenness_centrality(G)\n",
    "\n",
    "    # Calculating DAG-specific measures (ancestral and descendant sets)\n",
    "    ancestral_set_size = {node: len(nx.ancestors(G, node)) for node in G.nodes()}\n",
    "    descendant_set_size = {node: len(nx.descendants(G, node)) for node in G.nodes()}\n",
    "\n",
    "    # Calculating Hub and Authority Scores\n",
    "    hubs, authorities = nx.hits(G, max_iter=1000)\n",
    "\n",
    "    # Creating subplots\n",
    "    fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(15, 18))\n",
    "    centrality_measures = [in_degree_centrality, out_degree_centrality, closeness_centrality, betweenness_centrality, hubs, authorities]\n",
    "    titles = ['In-Degree Centrality', 'Out-Degree Centrality', 'Closeness Centrality', 'Betweenness Centrality', 'Hub Scores', 'Authority Scores']\n",
    "\n",
    "    # Plotting each centrality measure as a horizontal bar plot\n",
    "    for i, ax in enumerate(axes.flatten()):\n",
    "        if i >= len(centrality_measures):\n",
    "            break  # Avoiding index error due to an extra subplot\n",
    "        \n",
    "        centrality = centrality_measures[i]\n",
    "        # Sorting the nodes based on centrality values\n",
    "        nodes_sorted = sorted(centrality.items(), key=lambda x: x[1], reverse=False)\n",
    "        nodes, values = zip(*nodes_sorted)\n",
    "\n",
    "        # Creating the bar plot\n",
    "        cmap = plt.get_cmap('RdBu')\n",
    "        colors = cmap(np.linspace(0, 1, len(nodes)))\n",
    "        ax.barh(nodes, values, color=colors)\n",
    "        ax.set_title(titles[i])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "        \n",
    "        # Function to split label into two lines if needed\n",
    "    def split_label_function(label, max_chars_per_line=15):\n",
    "        if len(label) <= max_chars_per_line:\n",
    "            return label\n",
    "        split_index = label.rfind(' ', 0, max_chars_per_line)\n",
    "        if split_index == -1:\n",
    "            return label  # No space found, return original label\n",
    "        return label[:split_index] + '\\n' + label[split_index + 1:]\n",
    "    \n",
    "    # Apply topological layout and format labels\n",
    "    for layer, nodes in enumerate(nx.topological_generations(G)):\n",
    "        for node in nodes:\n",
    "            G.nodes[node][\"layer\"] = layer\n",
    "    \n",
    "            # Reformatting and splitting labels\n",
    "            formatted_label = node.replace('_', ' ').title()\n",
    "            split_label = split_label_function(formatted_label)  # Use the function correctly\n",
    "            G.nodes[node][\"label\"] = split_label\n",
    "    \n",
    "    pos = nx.multipartite_layout(G, subset_key=\"layer\")\n",
    "    \n",
    "    # Draw the graph using the formatted and split labels\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    nx.draw(G, pos, labels=nx.get_node_attributes(G, 'label'), with_labels=True, \n",
    "            node_size=4000, node_color='white', font_size=8, font_weight='regular', \n",
    "            edgecolors='black', linewidths=4, arrowstyle='-|>', arrowsize=20)\n",
    "    plt.title('Estimated Causal Graph (Data + Prior Information)')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    # Draw the graph using the spring layout\n",
    "    pos_spring = nx.spring_layout(G)\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    nx.draw(G, pos_spring, labels=nx.get_node_attributes(G, 'label'), with_labels=True, \n",
    "            node_size=4000, node_color='lightblue', font_size=8, font_weight='regular', \n",
    "            edgecolors='black', linewidths=4, arrowstyle='-|>', arrowsize=20)\n",
    "    plt.title('Estimated Causal Graph (Data + Prior Information) - Spring Layout')\n",
    "    plt.show()\n",
    "\n",
    "    # Print Clustering Coefficient and Assortativity\n",
    "    clustering_coefficient = nx.average_clustering(G)\n",
    "    assortativity = nx.degree_assortativity_coefficient(G)\n",
    "    print(f\"Average Clustering Coefficient: {clustering_coefficient:.2f}\")\n",
    "    print(f\"Assortativity Coefficient: {assortativity:.2f}\")\n",
    "\n",
    "    # Check if G is a DAG and print the result\n",
    "    is_dag = nx.is_directed_acyclic_graph(G)\n",
    "    print(f\"Is the Graph a DAG?: {is_dag}\")\n",
    "\n",
    "    # If not a DAG, count and print the number of cycles\n",
    "    if not nx.is_directed_acyclic_graph(G):\n",
    "        cycle_count = len(list(nx.simple_cycles(G)))\n",
    "        print(f\"Number of Cycles: {cycle_count}\")\n",
    "\n",
    "        # Convert networkx graph to igraph\n",
    "        ig_G = ig.Graph.from_networkx(G)\n",
    "\n",
    "        # Store node names as attributes in igraph\n",
    "        ig_G.vs[\"name\"] = list(G.nodes())\n",
    "\n",
    "        # Find the Feedback Arc Set using igraph\n",
    "        fas = ig_G.feedback_arc_set()\n",
    "        print(f\"Minimum number of edges to remove to make G a DAG: {len(fas)}\")\n",
    "\n",
    "        # Print the edges that need to be removed\n",
    "        print(\"Edges to be removed:\")\n",
    "        for edge in fas:\n",
    "            source, target = ig_G.es[edge].tuple\n",
    "            source_name = ig_G.vs[source][\"name\"]\n",
    "            target_name = ig_G.vs[target][\"name\"]\n",
    "            print(f\"({source_name}, {target_name})\")\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    parents = list(G.predecessors(outcome))\n",
    "    print(f\"Parents of {outcome}:\")\n",
    "    for parent in parents:\n",
    "        print(f\"  {parent}\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "    # Print all ancestors of the outcome node, each on a new line\n",
    "    ancestors = list(nx.ancestors(G, outcome))\n",
    "    print(f\"Ancestors of {outcome}:\")\n",
    "    for ancestor in ancestors:\n",
    "        print(f\"  {ancestor}\")\n",
    "\n",
    "\n",
    "graph_dag(G, 'mrr_converted')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba6f0bc-8195-4ed4-a7e4-14566141fea2",
   "metadata": {},
   "source": [
    "# Identification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b474d9-3770-4028-a53e-d1b0f0a7ca74",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def dowhy_inference(data, outcome, treatment, G, transform):\n",
    "    import numpy as np\n",
    "    import scipy.stats as stats  \n",
    "    import networkx as nx\n",
    "    from io import StringIO\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    from dowhy import CausalModel\n",
    "    from econml.dml import SparseLinearDML, DML, CausalForestDML, NonParamDML, KernelDML\n",
    "    from econml.metalearners import TLearner, SLearner, XLearner, DomainAdaptationLearner\n",
    "    from econml.inference import BootstrapInference\n",
    "    \n",
    "    from sklearn.linear_model import LinearRegression, LassoCV\n",
    "    from sklearn.metrics import mean_absolute_percentage_error\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, GradientBoostingRegressor\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "    \n",
    "\n",
    "# =============================================================================\n",
    "# Make NetworkX graph a GML    \n",
    "# =============================================================================\n",
    "    \n",
    "    G_gml = \"\\n\".join(nx.generate_gml(G))\n",
    "    \n",
    "    # Create a copy of the data to avoid modifying the original DataFrame\n",
    "    data_copy = data.copy()\n",
    "    \n",
    "    \n",
    "    # Data Transformation with Log or LogHalf\n",
    "    if transform == 'log':\n",
    "        data_copy[outcome] = np.log1p(data_copy[outcome])\n",
    "        data_copy[treatment] = np.log1p(data_copy[treatment])\n",
    "    elif transform == 'loghalf':\n",
    "        data_copy[outcome] = np.log(data_copy[outcome] + 0.5)\n",
    "        data_copy[treatment] = np.log(data_copy[treatment] + 0.5)\n",
    "    \n",
    "    for col in data_copy.columns:\n",
    "        if col not in [outcome, treatment] and data_copy[col].nunique() > 2:\n",
    "            if transform == 'log':\n",
    "                data_copy[col] = np.log1p(data_copy[col])\n",
    "            elif transform == 'loghalf':\n",
    "                data_copy[col] = np.log(data_copy[col] + 0.5)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Make model\n",
    "# =============================================================================\n",
    "    \n",
    "    model = CausalModel(\n",
    "    data = data_copy,\n",
    "    treatment = treatment,\n",
    "    outcome = outcome,\n",
    "    graph = G_gml\n",
    "    )       \n",
    "\n",
    "    model.view_model()\n",
    "\n",
    "# =============================================================================\n",
    "# Refute model\n",
    "# =============================================================================\n",
    "\n",
    "    # refuter_object = model.refute_graph(k=1, independence_test = \n",
    "    #                                     {'test_for_continuous': 'partial_correlation', \n",
    "    #                                       'test_for_discrete' : 'conditional_mutual_information'})\n",
    "    # print(refuter_object)\n",
    "\n",
    "# =============================================================================\n",
    "# Identify estimand\n",
    "# =============================================================================\n",
    "    identified_estimand = model.identify_effect()\n",
    "    print(identified_estimand)\n",
    " \n",
    "\n",
    "# =============================================================================\n",
    "# Estimate    \n",
    "# =============================================================================\n",
    "\n",
    "    estimate = model.estimate_effect(identified_estimand,\n",
    "                                          method_name=\"backdoor.econml.dml.CausalForestDML\",\n",
    "                                          target_units = 'ate', \n",
    "                                          confidence_intervals=True,\n",
    "                                          effect_modifiers =   [# Define the specific covariates you want to include in X\n",
    "                                                'seniority_Department lead', 'seniority_Executive', 'seniority_Individual contributor', \n",
    "                                                'seniority_Team lead', 'role_CUSTOMER_SUCCESS', 'role_DESIGN', 'role_ENGINEERING', \n",
    "                                                'role_FINANCE', 'role_LEGAL', 'role_MANAGEMENT', 'role_MARKETING', 'role_OPERATIONS', \n",
    "                                                'role_OTHER', 'role_PRODUCT_MANAGEMENT', 'role_RESEARCH', 'role_SALES', \n",
    "                                                'role_STUDENT', 'role_SUPPORT', 'tag_count_f7d', 'reel_created_count_f7d'],\n",
    "                                          method_params={\"init_params\":{'model_y':GradientBoostingRegressor(),\n",
    "                                                                  'model_t': GradientBoostingRegressor(),\n",
    "                                                                  'featurizer':PolynomialFeatures(degree=1, include_bias=True),\n",
    "                                                                  'cv': 4},\n",
    "                                                    \"fit_params\":{\n",
    "                                                                    'inference': BootstrapInference(n_bootstrap_samples=100, n_jobs=-1),\n",
    "                                                                }\n",
    "                                                  })\n",
    "    cate = estimate.cate_estimates\n",
    "    print(estimate)\n",
    "    print(cate)\n",
    "    \n",
    "\n",
    "# =============================================================================\n",
    "# Refute estimates\n",
    "# =============================================================================\n",
    "    res_placebo = model.refute_estimate(identified_estimand, estimate,\n",
    "            method_name=\"placebo_treatment_refuter\", show_progress_bar=True, n_jobs = -1, placebo_type=\"permute\")\n",
    "    \n",
    "    \n",
    "    print(res_placebo)\n",
    "    \n",
    "    \n",
    "    # res_subset = model.refute_estimate(\n",
    "    # identified_estimand, estimate,\n",
    "    # method_name=\"data_subset_refuter\",\n",
    "    # n_jobs=-1)\n",
    "    \n",
    "    # print(res_subset)\n",
    "    \n",
    "    \n",
    "    # res_random = model.refute_estimate(\n",
    "    # identified_estimand, estimate,\n",
    "    # method_name=\"random_common_cause\",\n",
    "    # n_jobs=-1\n",
    "    # )\n",
    "    # print(res_random)\n",
    "    \n",
    "    # res_bootstrap = model.refute_estimate(\n",
    "    # identified_estimand, estimate,\n",
    "    # method_name=\"bootstrap_refuter\",\n",
    "    # num_simulations=1000,  # Number of bootstrap simulations\n",
    "    # n_jobs=-1\n",
    "    # )\n",
    "    # print(res_bootstrap)\n",
    "    \n",
    "    \n",
    "    print(res_placebo)\n",
    "    # print(res_subset)\n",
    "    # print(res_random)\n",
    "    # print(res_bootstrap)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "#   Return\n",
    "# =============================================================================\n",
    "    return estimate, identified_estimand\n",
    "\n",
    "\n",
    "# Example usage of the function\n",
    "estimate, identified_estimand = dowhy_inference(data, 'mrr_converted', 'invite_count_f7d', G_priors, transform='loghalf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0766fee4-e4d7-4c99-8441-939270e99b4b",
   "metadata": {},
   "source": [
    "# Estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4258fe5-d3c8-457f-812c-b98de97a928d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def econml_dml_inference(data, outcome, treatment, estimator,  transform):\n",
    "    import numpy as np\n",
    "    import scipy.stats as stats\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.linear_model import LinearRegression, LassoCV\n",
    "    from sklearn.metrics import mean_absolute_percentage_error\n",
    "    from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, GradientBoostingRegressor\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    \n",
    "    from econml.dml import DML, LinearDML, SparseLinearDML, CausalForestDML\n",
    "    # from econml.cate_interpreter import SingleTreeCateInterpreter\n",
    "    # from econml.cate_interpreter import SingleTreePolicyInterpreter\n",
    "    # import shap\n",
    "    \n",
    "    \n",
    "# =============================================================================\n",
    "#     Data preprocessing\n",
    "# =============================================================================\n",
    "    \n",
    "    # Create a copy of the data to avoid modifying the original DataFrame\n",
    "    data_copy = data.copy()\n",
    "    \n",
    "    # Data Transformation with Log\n",
    "    if transform == 'log':\n",
    "        data_copy[outcome] = np.log1p(data_copy[outcome])\n",
    "        data_copy[treatment] = np.log1p(data_copy[treatment])\n",
    "        for col in data_copy.columns:\n",
    "            if col not in [outcome, treatment] and data_copy[col].nunique() > 2:\n",
    "                data_copy[col] = np.log1p(data_copy[col])\n",
    "    \n",
    "    # New Transformation with Loghalf\n",
    "    elif transform == 'loghalf':\n",
    "        data_copy[outcome] = np.log1p(data_copy[outcome]) + 0.5\n",
    "        data_copy[treatment] = np.log1p(data_copy[treatment]) + 0.5\n",
    "        for col in data_copy.columns:\n",
    "            if col not in [outcome, treatment] and data_copy[col].nunique() > 2:\n",
    "                data_copy[col] = np.log1p(data_copy[col]) + 0.5\n",
    "\n",
    "                \n",
    "\n",
    "# =============================================================================\n",
    "# Create train and test data\n",
    "# =============================================================================\n",
    "    \n",
    "    # Define the specific covariates you want to include in X\n",
    "    interaction_variables = [\n",
    "        'seniority_Department lead', 'seniority_Executive', 'seniority_Individual contributor', \n",
    "        'seniority_Team lead', 'role_CUSTOMER_SUCCESS', 'role_DESIGN', 'role_ENGINEERING', \n",
    "        'role_FINANCE', 'role_LEGAL', 'role_MANAGEMENT', 'role_MARKETING', 'role_OPERATIONS', \n",
    "        'role_OTHER', 'role_PRODUCT_MANAGEMENT', 'role_RESEARCH', 'role_SALES', \n",
    "        'role_STUDENT', 'role_SUPPORT', 'tag_count_f7d', 'reel_created_count_f7d'\n",
    "    ]\n",
    "    \n",
    "    # interaction_variables = [\n",
    "    #     'highlight_count_f7d', 'tag_count_f7d', 'insight_count_f7d', 'reel_created_count_f7d', 'shared_object_note_count_f7d',\n",
    "    #     'shared_object_insight_count_f7d'\n",
    "    # ]\n",
    "    \n",
    "    # Extracting Y, T, and X with only the specified covariates\n",
    "    Y = data_copy[outcome]\n",
    "    T = data_copy[treatment]\n",
    "    X = data_copy[interaction_variables]  # Only include the desired covariates in X\n",
    "    \n",
    "    # List all columns in data_copy\n",
    "    all_columns = data_copy.columns.tolist()\n",
    "    \n",
    "    # Remove the columns that are in outcome, treatment, and confounders\n",
    "    W_columns = [col for col in all_columns if col not in [outcome, treatment] + interaction_variables]\n",
    "    \n",
    "    # Create W with the remaining columns\n",
    "    W = data_copy[W_columns]\n",
    "    \n",
    "    # Split data into train-validation\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_test, T_train, T_test, Y_train, Y_test, W_train, W_test = train_test_split(X, T, Y, W, test_size=0.5)\n",
    "\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Reverse engineer graph representation\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "    import networkx as nx\n",
    "    \n",
    "    # Initialize the directed graph\n",
    "    G_priors = nx.DiGraph()\n",
    "    \n",
    "    # Add nodes for each type of variable\n",
    "    G_priors.add_node(outcome)  # Outcome variable\n",
    "    G_priors.add_node(treatment)  # Treatment variable\n",
    "    \n",
    "    # Add nodes for covariates with their actual names\n",
    "    for var in interaction_variables:\n",
    "        G_priors.add_node(var)\n",
    "    \n",
    "    # Add nodes for confounders/other variables with their actual names\n",
    "    for var in W_columns:\n",
    "        G_priors.add_node(var)\n",
    "    \n",
    "    # Add edges based on the relationships\n",
    "    G_priors.add_edge(treatment, outcome)  # Treatment affects outcome\n",
    "    \n",
    "    # Add edges for covariates affecting outcome and treatment\n",
    "    for var in interaction_variables:\n",
    "        G_priors.add_edge(var, outcome)  # Covariate affects outcome\n",
    "        G_priors.add_edge(var, treatment)  # Covariate may affect treatment\n",
    "    \n",
    "    # Add edges for confounders affecting outcome and treatment\n",
    "    for var in W_columns:\n",
    "        G_priors.add_edge(var, outcome)  # Confounder affects outcome\n",
    "        G_priors.add_edge(var, treatment)  # Confounder may affect treatment\n",
    "    \n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Set up estimators\n",
    "# =============================================================================\n",
    "    \n",
    "#Cross validation\n",
    "    rf_reg = lambda: GridSearchCV(\n",
    "                estimator=RandomForestRegressor(),\n",
    "                param_grid={\n",
    "                        'max_depth': [5, 10, 15, None],\n",
    "                        'n_estimators': (10, 30, 50, 100, 200),\n",
    "                        'max_features': (1,2,3)\n",
    "                    }, cv=10, n_jobs=-1, scoring='neg_mean_squared_error'\n",
    "                )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    gb_reg = lambda: GridSearchCV(\n",
    "                estimator=GradientBoostingRegressor(),\n",
    "                param_grid={\n",
    "                    'n_estimators': [100, 200, 300],\n",
    "                    'learning_rate': [0.01, 0.1, 0.2],\n",
    "                    'max_depth': [3, 4, 5],\n",
    "                    'min_samples_split': [2, 4, 6],\n",
    "                    'min_samples_leaf': [1, 2, 4],\n",
    "                    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "                    'subsample': [0.8, 0.9, 1.0]\n",
    "                },\n",
    "                cv=5,\n",
    "                n_jobs=-1,\n",
    "                scoring='neg_mean_squared_error',\n",
    "                verbose=1\n",
    "            )\n",
    "\n",
    "\n",
    "#Train\n",
    "    if estimator == 'linear':\n",
    "        est = LinearDML(model_y = GradientBoostingRegressor(),\n",
    "                        model_t = GradientBoostingRegressor())\n",
    "    elif estimator == 'sparse':\n",
    "        est = SparseLinearDML(model_y = GradientBoostingRegressor(),\n",
    "                              model_t = GradientBoostingRegressor(),\n",
    "                              featurizer = PolynomialFeatures(degree=2),  # Modify degree as needed\n",
    "                              random_state = 123)\n",
    "    elif estimator == 'forest':\n",
    "        est = CausalForestDML(model_y = GradientBoostingRegressor(),\n",
    "                              model_t = GradientBoostingRegressor(),\n",
    "                              featurizer = PolynomialFeatures(degree=2),  # Modify degree as needed\n",
    "                              criterion ='mse', n_estimators = 1000,\n",
    "                              min_impurity_decrease = 0.001, random_state = 0)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid estimator. Choose 'linear', 'sparse', or 'forest'.\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Train model on estimator\n",
    "# =============================================================================\n",
    "\n",
    "    # Fit the model\n",
    "    est.fit(Y_train, T_train, X=X_train, W = W_train)\n",
    "\n",
    "\n",
    "    # Get CATE estimates and confidence intervals\n",
    "    cate_estimates = est.effect(X_test)\n",
    "    lb, ub = est.effect_interval(X_test, alpha=0.05)\n",
    "    \n",
    "    # Assuming X_test is a NumPy array or similar, convert it to a DataFrame\n",
    "    results = pd.DataFrame(X_test)\n",
    "    \n",
    "        # Add the CATE estimates and the bounds as new columns\n",
    "    results['CATE_Estimates'] = cate_estimates\n",
    "    results['Lower_Bound'] = lb\n",
    "    results['Upper_Bound'] = ub\n",
    "    results['invite_count_f7d'] = T_test\n",
    "    results['mrr_converted'] = Y_test\n",
    "    \n",
    "    \n",
    "# =============================================================================\n",
    "# Tree interpreter\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "    # est.fit(Y, T, X=X, W=W)\n",
    "    # intrp = SingleTreeCateInterpreter(include_model_uncertainty=True, max_depth=2, min_samples_leaf=10)\n",
    "    \n",
    "    # # We interpret the CATE model's behavior based on the features used for heterogeneity\n",
    "    # intrp.interpret(est, X)\n",
    "    \n",
    "    # # Plot the tree\n",
    "    # plt.figure(figsize=(25, 5))\n",
    "    # intrp.plot(feature_names = interaction_variables, fontsize=12)\n",
    "    # plt.show()\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Policy interepreter\n",
    "# =============================================================================\n",
    "    \n",
    "\n",
    "    # # We find a tree-based treatment policy based on the CATE model\n",
    "    # # sample_treatment_costs is the cost of treatment. Policy will treat if effect is above this cost.\n",
    "    # intrp = SingleTreePolicyInterpreter(risk_level=None, max_depth=2, min_samples_leaf=1,min_impurity_decrease=.001)\n",
    "    # intrp.interpret(est, X, sample_treatment_costs=0.02)\n",
    "    # # Plot the tree\n",
    "    # intrp.plot(feature_names=[interaction_variables], fontsize=12)\n",
    "\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SHAP values\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "    # shap_values = est.shap_values(X)\n",
    "    # shap.summary_plot(shap_values['Y0']['T0'])\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Plotting    \n",
    "# =============================================================================\n",
    "\n",
    "# Assuming 'results' DataFrame and necessary variables (treatment, outcome) are already defined\n",
    "\n",
    "    sns.set(style=\"darkgrid\")\n",
    "\n",
    "# =============================================================================\n",
    "# Line Plot for CATE Estimates\n",
    "# =============================================================================\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for column in results.columns:\n",
    "        if column != 'invite_count_f7d' and results[column].nunique() == 2:\n",
    "            subset = results[results[column] == 1]\n",
    "            sns.lineplot(x='invite_count_f7d', y='CATE_Estimates', data=subset, label=column)\n",
    "    \n",
    "    plt.xlabel('Invite Count (7 days)')\n",
    "    plt.ylabel('$ Uplift per Invite')\n",
    "    plt.title('CATE Estimates by Treatment and Dummy Variables')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# Box Plot for CATE Estimates by Segment\n",
    "# =============================================================================\n",
    "\n",
    "# Reshape for Box Plot\n",
    "    melted_results = pd.melt(results, id_vars=['CATE_Estimates'], value_vars=[col for col in results.columns if col not in ['CATE_Estimates', 'invite_count_f7d', 'Lower_Bound', 'Upper_Bound']], \n",
    "                                 var_name='Dummy_Variable', value_name='Value')\n",
    "    \n",
    "    # Filter out rows where Value is 0\n",
    "    melted_results = melted_results[melted_results['Value'] == 1]\n",
    "    \n",
    "    # Calculate medians and sort\n",
    "    medians = melted_results.groupby('Dummy_Variable')['CATE_Estimates'].median().sort_values(ascending=False)\n",
    "    sorted_dummies = medians.index.tolist()\n",
    "    \n",
    "    # Calculate the Average Treatment Effect (ATE)\n",
    "    ATE = results['CATE_Estimates'].mean() / 10  # Dividing by 10 for elasticity\n",
    "    \n",
    "    # Format the ATE as a percentage with no decimal places\n",
    "    ATE_percent = f\"{ATE:.0%}\"\n",
    "    \n",
    "    # Format treatment and outcome names\n",
    "    formatted_treatment = treatment.replace('_', ' ').title()\n",
    "    formatted_outcome = outcome.replace('_', ' ').title()\n",
    "    \n",
    "    # New plot title\n",
    "    new_title = f\"A 10% increase in {formatted_treatment} drives a {ATE_percent} uplift in converted revenue\"\n",
    "    \n",
    "    # Creating the box plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.boxplot(y='Dummy_Variable', x='CATE_Estimates', data=melted_results, palette=\"Set2\", orient='h', order=sorted_dummies)\n",
    "    \n",
    "    plt.ylabel('Segment')\n",
    "    plt.xlabel('$ Revenue Uplift (Elasticity)')\n",
    "    plt.title(new_title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    \n",
    "# =============================================================================\n",
    "#  Return graph and results   \n",
    "# =============================================================================\n",
    "\n",
    "    return G_priors,  results\n",
    "\n",
    "\n",
    "\n",
    "G_priors, results = econml_dml_inference(data, 'mrr_converted', 'invite_count_f7d', estimator = 'forest', transform = 'loghalf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68eefb38-bfc9-4764-a993-d56e9e471e1c",
   "metadata": {},
   "source": [
    "# Interpret results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33dec8f-0056-4110-b08b-953e2386117f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#%%\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def interpret(results, treatment, outcome, total_treatments=6000):\n",
    "    # Ensure the necessary columns are present\n",
    "    required_columns = ['CATE_Estimates', treatment, outcome]\n",
    "    if not all(col in results.columns for col in required_columns):\n",
    "        raise ValueError(\"Missing required columns in the results dataframe.\")\n",
    "\n",
    "    # Histogram of CATE_Estimates\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(results['CATE_Estimates'], kde=True)\n",
    "    plt.title(\"Histogram of CATE Estimates\")\n",
    "    plt.xlabel(\"CATE Estimates\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()\n",
    "    \n",
    "        # CDF of CATE_Estimates\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(results['CATE_Estimates'], kde=True, cumulative=True, stat=\"density\")\n",
    "    plt.title(\"CDF of CATE Estimates\")\n",
    "    plt.xlabel(\"CATE Estimates\")\n",
    "    plt.ylabel(\"Cumulative Density\")\n",
    "    plt.show()\n",
    "\n",
    "    # Scatter Plot - Treatment vs CATE Estimates\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(x=results[treatment], y=results['CATE_Estimates'])\n",
    "    plt.title(\"Scatter Plot of Treatment vs CATE Estimates\")\n",
    "    plt.xlabel(\"Treatment\")\n",
    "    plt.ylabel(\"CATE Estimates\")\n",
    "    plt.show()\n",
    "\n",
    "    # Scatter Plot - Outcome vs CATE Estimates\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(x=results[outcome], y=results['CATE_Estimates'])\n",
    "    plt.title(\"Scatter Plot of Outcome vs CATE Estimates\")\n",
    "    plt.xlabel(\"Outcome\")\n",
    "    plt.ylabel(\"CATE Estimates\")\n",
    "    plt.show()\n",
    "\n",
    "    # Qini Curve\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    qini_curve(results, treatment, outcome, total_treatments)\n",
    "    plt.title(\"Qini Curve\")\n",
    "    plt.xlabel(\"Proportion Targeted\")\n",
    "    plt.ylabel(\"Cumulative Uplift\")\n",
    "    plt.show()\n",
    "\n",
    "def qini_curve(df, treatment, outcome, total_treatments):\n",
    "    df_sorted = df.sort_values(by='CATE_Estimates', ascending=False)\n",
    "    df_sorted['cumulative_treatment'] = df_sorted[treatment].cumsum()\n",
    "    df_sorted['cumulative_control'] = (1 - df_sorted[treatment]).cumsum()\n",
    "    df_sorted['cumulative_outcome_treatment'] = df_sorted[outcome].cumsum()\n",
    "    df_sorted['cumulative_outcome_control'] = (1 - df_sorted[outcome]).cumsum()\n",
    "\n",
    "    df_sorted['uplift'] = df_sorted['cumulative_outcome_treatment'] - (df_sorted['cumulative_outcome_control'] * df_sorted['cumulative_treatment'] / df_sorted['cumulative_control'])\n",
    "    df_sorted['uplift'] = df_sorted['uplift'] * total_treatments / len(df_sorted)\n",
    "    df_sorted['proportion'] = np.arange(1, len(df_sorted) + 1) / len(df_sorted)\n",
    "\n",
    "    plt.plot(df_sorted['proportion'], df_sorted['uplift'], label=\"Qini Curve\")\n",
    "    plt.plot([0, 1], [0, df_sorted['uplift'].max()], linestyle='--', color='red', label=\"Random\")\n",
    "\n",
    "# Example usage\n",
    "# interpret(results, 'invite_count_f7d', 'mrr_converted', 6000)\n",
    "\n",
    "interpret(results, 'invite_count_f7d', 'mrr_converted')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (EconML)",
   "language": "python",
   "name": "econml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
