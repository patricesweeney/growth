{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7fa5765-659a-4375-8fe4-593666cb934e",
   "metadata": {},
   "source": [
    "# Value-based bidding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f70f79c-ff37-409b-877d-3eade934b400",
   "metadata": {},
   "source": [
    "Value-based bidding is a strategy where advertisers set bids based on the estimated value each ad click brings to their business, aiming to optimize return on investment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9967ef05",
   "metadata": {},
   "source": [
    "In performance marketing, ad bidding involves setting a price you're willing to pay for specific actions or outcomes from your ads, such as clicks, conversions, or impressions. \n",
    "\n",
    "To determine if your ad bidding strategy is profitable and to price your bid optimally for revenue, the primary goal is to ensure that the cost of acquiring a customer (CAC) through ads is less than the customer's lifetime value (LTV) to your business. This is not a faddish metric, and it follows directly from the basic profit equation.\n",
    "\n",
    "$$ \\text{Profitable} \\iff LTV_i > CAC_i $$\n",
    "\n",
    "CAC includes all marketing and sales expenses over a specific period. This means advertising spend, salaries of marketing and sales teams, software costs, creative costs, and more.\n",
    "\n",
    "In practice, CPA is more commonly used, because it's easy to forget about fixed costs.  While CAC includes a wider range of costs (marketing, sales, overheads), whereas CPA is typically confined to direct campaign costs.\n",
    "\n",
    "$$ \\text{Maybe profitable} \\iff LTV_i > CPA_i $$\n",
    "\n",
    "\n",
    "Return on ad-spend (ROAS) can be thought of as a myopic LTV. ROAS measures the immediate effectiveness of specific advertising campaigns in generating revenue.  ROAS is calculated by dividing the revenue generated from a particular advertising campaign by the cost of that campaign. The time cutoff depends on 'revenue generated' depends on the specific goals and context of the advertising campaign. There is no standard, universally applicable time frame. As a result, it is a vague and static metric. Sometimes, 'liftime ROAS' is used as a synonym for a modified LTV/CPA ratio which takes into account the lifetime revenue generated.\n",
    "\n",
    "$$\\text{Lifetime ROAS} = \\dfrac{LTV}{CPA} - 1 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4dceaba",
   "metadata": {},
   "source": [
    "# Bidding strategies\n",
    "\n",
    "Broadly speaking, bidding strategies break into automated and manual.\n",
    "\n",
    "### Automated bidding strategies\n",
    "\n",
    "Within automated bidding strategies, we break again into maximisation and targeting.\n",
    "\n",
    "Maximisation strategies aim to maximize a specific aspect of your campaign, such as clicks or conversions, within your budget.\n",
    "\n",
    "- **Maximize Clicks:** Focuses on driving as many clicks as possible to your website within your set budget.\n",
    "- **Maximize Conversions:** Aims to get as many conversions as possible within your specified budget.\n",
    "- **Maximize Conversion Value:** Seeks to maximize the total conversion value (like revenue) instead of the number of conversions, within your budget.\n",
    "\n",
    "Targeting strategies are focused on achieving a specific target, such as a desired CPA or ROAS.\n",
    "\n",
    "- **Target Impression Share:** Sets bids to achieve a specified impression share percentage on the search result page.\n",
    "- **tCPA (Target Cost Per Acquisition):** Sets bids to target an average CPA that you specify. This strategy tries to get as many conversions as possible at the set target CPA.\n",
    "- **tROAS (Target Return On Ad Spend):** Aims to achieve a target return on ad spend. It automatically sets bids to maximize conversion value while trying to reach the target ROAS.\n",
    "\n",
    "\n",
    "### Manual bidding strategies\n",
    "- **Manual CPC:** Direct control over bids for clicks.\n",
    "- **eCPC (Enhanced CPC):** Allows some automatic adjustment for clicks likely to lead to sales or conversions.\n",
    "\n",
    "Automated bidding strategies, specifically Maximise Conversion Value (MCV) and Target ROAS (tROAS) are the closest to a value-based bidding strategy. \n",
    "- In MCV, the bidding algorithm may aggressively pursue high-value conversions, even if they come at a higher cost, as long as they contribute to the overall revenue. \n",
    "- In tROAS, the algorithm adjusts bids to meet the specific return ratio. This might involve passing up on certain high-cost conversions if they don't meet the ROAS lower bound, potentially leading to different bidding decisions compared to purely maximizing revenue. \n",
    "\n",
    "While both strategies aim to maximize revenue, MCV does so without the constraint of a specific return ratio, whereas tROAS seeks to balance revenue maximization with a lower bound on ROAS. The distinction becomes more pronounced based on how the target ROAS is set relative to the maximum achievable ROAS."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9218b8f2-1fac-4ea3-9110-78be59e3ebf6",
   "metadata": {},
   "source": [
    "### Best-in-class solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd793f2-db84-4164-8410-526109fb2495",
   "metadata": {},
   "source": [
    "We define value using CLV, the present value of revenue flows:\n",
    "\n",
    "$$\\text{CLV}_i = \\frac{P_i \\times V_i \\times r_i}{1 + \\text{WACC} - r_i}$$\n",
    "\n",
    "- $P_i$ is price per seat\n",
    "- $V_i$ is seat volume\n",
    "- $r_i$ is retention rate\n",
    "- $WACC$ is the weighted-average cost of capital\n",
    "\n",
    "We need to predict CLV for each customer in the first 24-hours of sign up so we can send that information to advertisers.\n",
    "\n",
    "Decomposing the uncertainty of $CLV$, the major assumption is that the majority of entropy is in whether the seat volume is > 0 or not. That is, whether or not people convert is the biggest unknown.\n",
    "\n",
    "**Tl;dr:**\n",
    "1. Calculate CLV using known information for current signups.\n",
    "2. Train regression model to predict CLV on unseen signups.\n",
    "\n",
    "**Calculating CLV using known information**\n",
    "- Revenue ($P_i \\cdot V_i$): current MRR\n",
    "- $r_i$: A function of package, with each package estimated using an exponential survival model\n",
    "- $WACC$: 15%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abcb214-5252-44fa-98b7-30560b0bac6f",
   "metadata": {},
   "source": [
    "### Good-enough solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e72e782-0d2e-4e1a-b94d-2a416493a515",
   "metadata": {},
   "source": [
    "We still define value as CLV, but we focus on predicting which product the person will convert to (if any).\n",
    "\n",
    "\n",
    "**Tl;dr:**\n",
    "1. Calculate CLV using known information for current signups.\n",
    "2. Train classification model to predict land package on unseen signups.\n",
    "3. Assign the mean CLV of current customers on that package to that package.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1dca360-3f56-4406-8022-3bbff888399d",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5df39cd-2731-4ef5-96aa-81a00cecef2e",
   "metadata": {},
   "source": [
    "**$y$ variable**\n",
    "- $CLV_i$\n",
    "- $Product_j$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee594254-a2e4-4bc6-aa46-2823bb89c7fa",
   "metadata": {},
   "source": [
    "**$x$ variables**\n",
    "- Segment [str]\n",
    "- Education email flag [str]\n",
    "- Company email flag [str]\n",
    "- Industry [str]\n",
    "- Revenue [str]\n",
    "- Employees [str]\n",
    "- City [str]\n",
    "- State [str]\n",
    "- Country [str]\n",
    "- Data residency [str]\n",
    "- GA signup flag [str]\n",
    "- FB signup flag [str]\n",
    "- Feature counts in first 1 day [int]\n",
    "\n",
    "Plus first party data\n",
    "- Checklist user completed % [production.onboarding_checklist_completed: user_id, workspace_id] (what proportion of users have completed checklist)\n",
    "- Step user conversion % [production.new_user_global_onboarding: step, workspace_id, user_id] (what proportion of users have onboarded)\n",
    "- Intent mix [production.onboarding_user_intent_selected: intent, workspace_id, user_id] (mix of users' intent with the product)\n",
    "- Data mix [production.onboarding_import_option_selected: option, workspace_id, user_id] (mix of users' readiness to import data)\n",
    "- Use case mix [production.onboarding_use_case_selected: use_case, workspace_id, user_id] (mix of users' job roles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed9d27e",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "WITH workspaces as (\n",
    "    SELECT workspace_id, \n",
    "        case\n",
    "    \t    when products_latest ilike '%Starter%' then 'Starter'\n",
    "    \t    when products_latest ilike '%Pro%' then 'Starter'\n",
    "    \t    when products_latest ilike '%Analysis%' then 'Starter'\n",
    "    \t    when products_latest ilike '%Team%' then 'Team'\n",
    "    \t    when products_latest ilike '%Business%' then 'Business'\n",
    "    \t    when products_latest ilike '%Enterprise%' then 'Enterprise'\n",
    "    \t    else 'No conversion' end as product,\n",
    "       COALESCE(clv, 0) AS clv, \n",
    "       COALESCE(workspace_segment, 'Unknown') AS segment,\n",
    "       CASE \n",
    "           WHEN company_domain_education_flag IS NULL THEN 'Unknown' \n",
    "           ELSE CASE company_domain_education_flag \n",
    "                WHEN TRUE THEN '1' \n",
    "                ELSE '0' \n",
    "                END \n",
    "       END AS education_flag,\n",
    "       CASE \n",
    "           WHEN company_email_provider IS NULL THEN 'Unknown' \n",
    "           ELSE CASE company_email_provider \n",
    "                WHEN TRUE THEN '1' \n",
    "                ELSE '0' \n",
    "                END \n",
    "       END AS company_email_flag,\n",
    "       COALESCE(company_industry_group, 'Unknown') AS industry,\n",
    "       COALESCE(company_estimated_annual_revenue, 'Unknown') AS revenue,\n",
    "       COALESCE(company_employees_range, 'Unknown') AS employees_range,\n",
    "       COALESCE(company_city, 'Unknown') AS city,\n",
    "       COALESCE(company_state, 'Unknown') AS state,\n",
    "       COALESCE(company_country_code, 'Unknown') AS country_code,\n",
    "       CASE \n",
    "           WHEN ga_signup_flag IS NULL THEN 'Unknown' \n",
    "           ELSE CASE ga_signup_flag \n",
    "                WHEN TRUE THEN '1' \n",
    "                ELSE '0' \n",
    "                END \n",
    "       END AS ga_signup_flag,\n",
    "       CASE \n",
    "           WHEN fb_signup_flag IS NULL THEN 'Unknown' \n",
    "           ELSE CASE fb_signup_flag \n",
    "                WHEN TRUE THEN '1' \n",
    "                ELSE '0' \n",
    "                END \n",
    "       END AS fb_signup_flag,\n",
    "       COALESCE(data_residency_region, 'Unknown') AS residency_region,\n",
    "       COALESCE(project_count_f1d, 0) AS project_count,\n",
    "       COALESCE(transcription_count_f1d, 0) AS transcription_count,\n",
    "       COALESCE(highlight_count_f1d, 0) AS highlight_count,\n",
    "       COALESCE(tag_count_f1d, 0) AS tag_count,\n",
    "       COALESCE(insight_count_f1d, 0) AS insight_count,\n",
    "       COALESCE(reel_created_count_f1d, 0) AS reel_created_count,\n",
    "       COALESCE(invite_count_f1d, 0) AS invite_count,\n",
    "       COALESCE(shared_object_note_count_f1d, 0) AS shared_object_note_count,\n",
    "       COALESCE(shared_object_insight_count_f1d, 0) AS shared_object_insight_count,\n",
    "       COALESCE(note_viewed_user_count_f1d, 0) AS note_viewed_user_count,\n",
    "       COALESCE(tag_viewed_user_count_f1d, 0) AS tag_viewed_user_count\n",
    "    FROM \"dbt_staging\".\"stg_workspace\"\n",
    "    where workspace_created_at >= '2023-01-01'\n",
    "),\n",
    "\n",
    "users as (\n",
    "SELECT \n",
    "    u.workspace_id, \n",
    "    SUM(CASE \n",
    "            WHEN created_at <= workspace_created_at + INTERVAL '1 day' THEN 1\n",
    "            ELSE 0 \n",
    "        END) as f1d_users\n",
    "FROM \n",
    "    \"dbt_staging\".\"stg_user\" u\n",
    "LEFT JOIN \n",
    "    \"dbt_staging\".\"stg_workspace\" w ON u.workspace_id = w.workspace_id\n",
    "GROUP BY \n",
    "    u.workspace_id\n",
    "),\n",
    "\n",
    "checklist_completed as (\n",
    "SELECT \n",
    "    oc.context_group_id as workspace_id, \n",
    "    oc.timestamp, \n",
    "    COUNT(DISTINCT oc.user_id) as onboarded_users\n",
    "FROM \n",
    "    \"production\".\"onboarding_checklist_completed\" oc\n",
    "JOIN \n",
    "    \"dbt_staging\".\"stg_workspace\" w ON oc.context_group_id = w.workspace_id\n",
    "WHERE \n",
    "    oc.timestamp <= w.workspace_created_at + INTERVAL '1 day'\n",
    "GROUP BY \n",
    "    oc.context_group_id, oc.timestamp\n",
    "\n",
    "),\n",
    "\n",
    "checklist_steps as (\n",
    "    SELECT \n",
    "        context_group_id as workspace_id,\n",
    "        timestamp::date as timestamp,\n",
    "        COUNT(DISTINCT CASE WHEN step = 'viewed_intro' THEN user_id ELSE NULL END) as viewed_intro,\n",
    "        COUNT(DISTINCT CASE WHEN step = 'set_role' THEN user_id ELSE NULL END) as set_role,\n",
    "        COUNT(DISTINCT CASE WHEN step = 'set_avatar' THEN user_id ELSE NULL END) as set_avatar,\n",
    "        COUNT(DISTINCT CASE WHEN step = 'invited_user' THEN user_id ELSE NULL END) as invited_user\n",
    "    FROM \n",
    "    production.new_user_global_onboarding\n",
    "    \n",
    "    GROUP BY \n",
    "    workspace_id, timestamp\n",
    "\n",
    "),\n",
    "\n",
    "intent as (\n",
    "SELECT \n",
    "    p.context_group_id as workspace_id,\n",
    "    p.timestamp::date as timestamp, \n",
    "    COUNT(DISTINCT CASE WHEN p.intent = 'VideoTranscription' THEN p.user_id ELSE NULL END) as intent_video_transcription,\n",
    "    COUNT(DISTINCT CASE WHEN p.intent = 'InsightsHub' THEN p.user_id ELSE NULL END) as intent_insights_hub,\n",
    "    COUNT(DISTINCT CASE WHEN p.intent = 'ResearchAnalysis' THEN p.user_id ELSE NULL END) as intent_research_analysis\n",
    "FROM \n",
    "    production.onboarding_user_intent_selected p\n",
    "JOIN \n",
    "    dbt_staging.stg_workspace w ON p.context_group_id = w.workspace_id\n",
    "WHERE \n",
    "    p.timestamp <= w.workspace_created_at + INTERVAL '1 day'\n",
    "GROUP BY  1,2\n",
    "\n",
    "),\n",
    "\n",
    "usecase as (\n",
    "SELECT \n",
    "    u.context_group_id as workspace_id,\n",
    "    u.timestamp::date as timestamp, \n",
    "    COUNT(DISTINCT CASE WHEN u.use_case = 'SALES' THEN u.user_id ELSE NULL END) as usecase_sales,\n",
    "    COUNT(DISTINCT CASE WHEN u.use_case = 'MARKETING' THEN u.user_id ELSE NULL END) as usecase_marketing,\n",
    "    COUNT(DISTINCT CASE WHEN u.use_case = 'PRODUCT_MANAGEMENT' THEN u.user_id ELSE NULL END) as usecase_product_management,\n",
    "    COUNT(DISTINCT CASE WHEN u.use_case = 'ENGINEERING' THEN u.user_id ELSE NULL END) as usecase_engineering,\n",
    "    COUNT(DISTINCT CASE WHEN u.use_case = 'OTHER' THEN u.user_id ELSE NULL END) as usecase_other,\n",
    "    COUNT(DISTINCT CASE WHEN u.use_case = 'MANAGEMENT' THEN u.user_id ELSE NULL END) as usecase_management,\n",
    "    COUNT(DISTINCT CASE WHEN u.use_case = 'DESIGN' THEN u.user_id ELSE NULL END) as usecase_design,\n",
    "    COUNT(DISTINCT CASE WHEN u.use_case = 'SUPPORT' THEN u.user_id ELSE NULL END) as usecase_support,\n",
    "    COUNT(DISTINCT CASE WHEN u.use_case = 'CUSTOMER_SUCCESS' THEN u.user_id ELSE NULL END) as usecase_customer_success,\n",
    "    COUNT(DISTINCT CASE WHEN u.use_case = 'RESEARCH' THEN u.user_id ELSE NULL END) as usecase_research,\n",
    "    COUNT(DISTINCT CASE WHEN u.use_case = 'OPERATIONS' THEN u.user_id ELSE NULL END) as usecase_operations,\n",
    "    COUNT(DISTINCT CASE WHEN u.use_case = 'LEGAL' THEN u.user_id ELSE NULL END) as usecase_legal,\n",
    "    COUNT(DISTINCT CASE WHEN u.use_case = 'FINANCE' THEN u.user_id ELSE NULL END) as usecase_finance\n",
    "FROM \n",
    "    production.onboarding_use_case_selected u\n",
    "JOIN \n",
    "    dbt_staging.stg_workspace w ON u.context_group_id = w.workspace_id\n",
    "WHERE \n",
    "    u.timestamp <= w.workspace_created_at + INTERVAL '1 day'\n",
    "GROUP BY \n",
    "1,2\n",
    "\n",
    "),\n",
    "\n",
    "data_upload as (\n",
    "SELECT \n",
    "    o.context_group_id as workspace_id,\n",
    "    o.timestamp::date as timestamp, \n",
    "    COUNT(DISTINCT CASE WHEN o.option = 'NoData' THEN o.user_id ELSE NULL END) as upload_nodata,\n",
    "    COUNT(DISTINCT CASE WHEN o.option = 'Data' THEN o.user_id ELSE NULL END) as upload_data\n",
    "FROM \n",
    "    production.onboarding_import_option_selected o\n",
    "JOIN \n",
    "    dbt_staging.stg_workspace w ON o.context_group_id = w.workspace_id\n",
    "WHERE \n",
    "    o.timestamp <= w.workspace_created_at + INTERVAL '1 day'\n",
    "GROUP BY 1,2\n",
    ")\n",
    "\n",
    "select *\n",
    "from workspaces w\n",
    "left join users on w.workspace_id = users.workspace_id\n",
    "left join checklist_completed on w.workspace_id = checklist_completed.workspace_id\n",
    "left join checklist_steps on w.workspace_id = checklist_steps.workspace_id\n",
    "left join intent on w.workspace_id = intent.workspace_id\n",
    "left join usecase on w.workspace_id = usecase.workspace_id\n",
    "left join data_upload on w.workspace_id = data_upload.workspace_id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78cac84-672f-4cbf-9d94-aacef0cff8b4",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5dcc42-d038-4501-96a2-b83bebbc49fb",
   "metadata": {},
   "source": [
    "### Import local XLSX file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eec7fd30-b48e-4cf6-98ce-62c4a18dbc61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/patricksweeney/anaconda3/envs/EconML/lib/python3.11/site-packages/openpyxl/styles/stylesheet.py:226: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
      "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>workspace_id</th>\n",
       "      <th>product</th>\n",
       "      <th>clv</th>\n",
       "      <th>segment</th>\n",
       "      <th>education_flag</th>\n",
       "      <th>company_email_flag</th>\n",
       "      <th>industry</th>\n",
       "      <th>revenue</th>\n",
       "      <th>employees_range</th>\n",
       "      <th>city</th>\n",
       "      <th>...</th>\n",
       "      <th>usecase_support</th>\n",
       "      <th>usecase_customer_success</th>\n",
       "      <th>usecase_research</th>\n",
       "      <th>usecase_operations</th>\n",
       "      <th>usecase_legal</th>\n",
       "      <th>usecase_finance</th>\n",
       "      <th>workspace_id.6</th>\n",
       "      <th>timestamp.4</th>\n",
       "      <th>upload_nodata</th>\n",
       "      <th>upload_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00095959-e65b-4256-aeba-06464ae106ac</td>\n",
       "      <td>No conversion</td>\n",
       "      <td>0.0</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Diversified Consumer Services</td>\n",
       "      <td>$50M-$100M</td>\n",
       "      <td>251-1K</td>\n",
       "      <td>Utrecht</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000b6e42-2fbd-45c0-9a73-de2cef1ece0d</td>\n",
       "      <td>No conversion</td>\n",
       "      <td>0.0</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Diversified Consumer Services</td>\n",
       "      <td>$1M-$10M</td>\n",
       "      <td>11-50</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000cdc27-c036-4702-9ceb-84ea9f806c3d</td>\n",
       "      <td>No conversion</td>\n",
       "      <td>0.0</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Diversified Consumer Services</td>\n",
       "      <td>$100M-$250M</td>\n",
       "      <td>251-1K</td>\n",
       "      <td>Clinton</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00190170-ab11-4a39-9a14-075e5d3c68ad</td>\n",
       "      <td>No conversion</td>\n",
       "      <td>0.0</td>\n",
       "      <td>FREE_EMAIL</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>001aa956-d9a4-46f0-a528-611936ff34fe</td>\n",
       "      <td>No conversion</td>\n",
       "      <td>0.0</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Diversified Consumer Services</td>\n",
       "      <td>$1B-$10B</td>\n",
       "      <td>1K-5K</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           workspace_id        product  clv     segment  \\\n",
       "0  00095959-e65b-4256-aeba-06464ae106ac  No conversion  0.0       OTHER   \n",
       "1  000b6e42-2fbd-45c0-9a73-de2cef1ece0d  No conversion  0.0       OTHER   \n",
       "2  000cdc27-c036-4702-9ceb-84ea9f806c3d  No conversion  0.0       OTHER   \n",
       "3  00190170-ab11-4a39-9a14-075e5d3c68ad  No conversion  0.0  FREE_EMAIL   \n",
       "4  001aa956-d9a4-46f0-a528-611936ff34fe  No conversion  0.0       OTHER   \n",
       "\n",
       "   education_flag company_email_flag                       industry  \\\n",
       "0               0                  0  Diversified Consumer Services   \n",
       "1               0                  0  Diversified Consumer Services   \n",
       "2               0                  0  Diversified Consumer Services   \n",
       "3               0                  1                        Unknown   \n",
       "4               1                  0  Diversified Consumer Services   \n",
       "\n",
       "       revenue employees_range     city  ... usecase_support  \\\n",
       "0   $50M-$100M          251-1K  Utrecht  ...             NaN   \n",
       "1     $1M-$10M           11-50   Mumbai  ...             NaN   \n",
       "2  $100M-$250M          251-1K  Clinton  ...             NaN   \n",
       "3      Unknown         Unknown  Unknown  ...             NaN   \n",
       "4     $1B-$10B           1K-5K  Unknown  ...             NaN   \n",
       "\n",
       "  usecase_customer_success  usecase_research  usecase_operations  \\\n",
       "0                      NaN               NaN                 NaN   \n",
       "1                      NaN               NaN                 NaN   \n",
       "2                      NaN               NaN                 NaN   \n",
       "3                      NaN               NaN                 NaN   \n",
       "4                      NaN               NaN                 NaN   \n",
       "\n",
       "  usecase_legal  usecase_finance  workspace_id.6  timestamp.4  upload_nodata  \\\n",
       "0           NaN              NaN             NaN          NaT            NaN   \n",
       "1           NaN              NaN             NaN          NaT            NaN   \n",
       "2           NaN              NaN             NaN          NaT            NaN   \n",
       "3           NaN              NaN             NaN          NaT            NaN   \n",
       "4           NaN              NaN             NaN          NaT            NaN   \n",
       "\n",
       "   upload_data  \n",
       "0          NaN  \n",
       "1          NaN  \n",
       "2          NaN  \n",
       "3          NaN  \n",
       "4          NaN  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def import_data():\n",
    "    import pandas as pd\n",
    "    file_path = '/Users/patricksweeney/growth/01_Acquisition/03_Value-based bidding/VBB Train 3.xlsx'\n",
    "    data = pd.read_excel(file_path)\n",
    "    return data\n",
    "\n",
    "data = import_data()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9806086-69da-4527-87b7-5cb04a007f65",
   "metadata": {},
   "source": [
    "### Check data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0e6308d-4753-4cbc-91b7-813fdb270c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features with missing values are:\n",
      "workspace_id: Missing Count = 0, Missing Percentage = 0.00%\n",
      "product: Missing Count = 0, Missing Percentage = 0.00%\n",
      "clv: Missing Count = 0, Missing Percentage = 0.00%\n",
      "segment: Missing Count = 0, Missing Percentage = 0.00%\n",
      "education_flag: Missing Count = 0, Missing Percentage = 0.00%\n",
      "company_email_flag: Missing Count = 0, Missing Percentage = 0.00%\n",
      "industry: Missing Count = 0, Missing Percentage = 0.00%\n",
      "revenue: Missing Count = 0, Missing Percentage = 0.00%\n",
      "employees_range: Missing Count = 0, Missing Percentage = 0.00%\n",
      "city: Missing Count = 0, Missing Percentage = 0.00%\n",
      "state: Missing Count = 0, Missing Percentage = 0.00%\n",
      "country_code: Missing Count = 0, Missing Percentage = 0.00%\n",
      "ga_signup_flag: Missing Count = 0, Missing Percentage = 0.00%\n",
      "fb_signup_flag: Missing Count = 0, Missing Percentage = 0.00%\n",
      "residency_region: Missing Count = 0, Missing Percentage = 0.00%\n",
      "project_count: Missing Count = 0, Missing Percentage = 0.00%\n",
      "transcription_count: Missing Count = 0, Missing Percentage = 0.00%\n",
      "highlight_count: Missing Count = 0, Missing Percentage = 0.00%\n",
      "tag_count: Missing Count = 0, Missing Percentage = 0.00%\n",
      "insight_count: Missing Count = 0, Missing Percentage = 0.00%\n",
      "reel_created_count: Missing Count = 0, Missing Percentage = 0.00%\n",
      "invite_count: Missing Count = 0, Missing Percentage = 0.00%\n",
      "shared_object_note_count: Missing Count = 0, Missing Percentage = 0.00%\n",
      "shared_object_insight_count: Missing Count = 0, Missing Percentage = 0.00%\n",
      "note_viewed_user_count: Missing Count = 0, Missing Percentage = 0.00%\n",
      "tag_viewed_user_count: Missing Count = 0, Missing Percentage = 0.00%\n",
      "workspace_id.1: Missing Count = 0, Missing Percentage = 0.00%\n",
      "f1d_users: Missing Count = 0, Missing Percentage = 0.00%\n",
      "workspace_id.2: Missing Count = 64593, Missing Percentage = 99.61%\n",
      "timestamp: Missing Count = 64593, Missing Percentage = 99.61%\n",
      "onboarded_users: Missing Count = 64593, Missing Percentage = 99.61%\n",
      "workspace_id.3: Missing Count = 64847, Missing Percentage = 100.00%\n",
      "timestamp.1: Missing Count = 64847, Missing Percentage = 100.00%\n",
      "viewed_intro: Missing Count = 64847, Missing Percentage = 100.00%\n",
      "set_role: Missing Count = 64847, Missing Percentage = 100.00%\n",
      "set_avatar: Missing Count = 64847, Missing Percentage = 100.00%\n",
      "invited_user: Missing Count = 64847, Missing Percentage = 100.00%\n",
      "workspace_id.4: Missing Count = 41330, Missing Percentage = 63.73%\n",
      "timestamp.2: Missing Count = 41330, Missing Percentage = 63.73%\n",
      "intent_video_transcription: Missing Count = 41330, Missing Percentage = 63.73%\n",
      "intent_insights_hub: Missing Count = 41330, Missing Percentage = 63.73%\n",
      "intent_research_analysis: Missing Count = 41330, Missing Percentage = 63.73%\n",
      "workspace_id.5: Missing Count = 62018, Missing Percentage = 95.64%\n",
      "timestamp.3: Missing Count = 62018, Missing Percentage = 95.64%\n",
      "usecase_sales: Missing Count = 62018, Missing Percentage = 95.64%\n",
      "usecase_marketing: Missing Count = 62018, Missing Percentage = 95.64%\n",
      "usecase_product_management: Missing Count = 62018, Missing Percentage = 95.64%\n",
      "usecase_engineering: Missing Count = 62018, Missing Percentage = 95.64%\n",
      "usecase_other: Missing Count = 62018, Missing Percentage = 95.64%\n",
      "usecase_management: Missing Count = 62018, Missing Percentage = 95.64%\n",
      "usecase_design: Missing Count = 62018, Missing Percentage = 95.64%\n",
      "usecase_support: Missing Count = 62018, Missing Percentage = 95.64%\n",
      "usecase_customer_success: Missing Count = 62018, Missing Percentage = 95.64%\n",
      "usecase_research: Missing Count = 62018, Missing Percentage = 95.64%\n",
      "usecase_operations: Missing Count = 62018, Missing Percentage = 95.64%\n",
      "usecase_legal: Missing Count = 62018, Missing Percentage = 95.64%\n",
      "usecase_finance: Missing Count = 62018, Missing Percentage = 95.64%\n",
      "workspace_id.6: Missing Count = 62975, Missing Percentage = 97.11%\n",
      "timestamp.4: Missing Count = 62975, Missing Percentage = 97.11%\n",
      "upload_nodata: Missing Count = 62975, Missing Percentage = 97.11%\n",
      "upload_data: Missing Count = 62975, Missing Percentage = 97.11%\n"
     ]
    }
   ],
   "source": [
    "def find_missing_values(data):\n",
    "    total_rows = len(data)\n",
    "    missing_values = data.isnull().sum()\n",
    "    missing_percentage = (missing_values / total_rows) * 100\n",
    "\n",
    "    print(\"Features with missing values are:\")\n",
    "    for feature, missing_count in missing_values.items():\n",
    "        percent = missing_percentage[feature]\n",
    "        print(f\"{feature}: Missing Count = {missing_count}, Missing Percentage = {percent:.2f}%\")\n",
    "\n",
    "# Usage\n",
    "find_missing_values(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054104b2",
   "metadata": {},
   "source": [
    "### Replace NaN with zero\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "228c0bd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>workspace_id</th>\n",
       "      <th>product</th>\n",
       "      <th>clv</th>\n",
       "      <th>segment</th>\n",
       "      <th>education_flag</th>\n",
       "      <th>company_email_flag</th>\n",
       "      <th>industry</th>\n",
       "      <th>revenue</th>\n",
       "      <th>employees_range</th>\n",
       "      <th>city</th>\n",
       "      <th>...</th>\n",
       "      <th>usecase_management</th>\n",
       "      <th>usecase_design</th>\n",
       "      <th>usecase_support</th>\n",
       "      <th>usecase_customer_success</th>\n",
       "      <th>usecase_research</th>\n",
       "      <th>usecase_operations</th>\n",
       "      <th>usecase_legal</th>\n",
       "      <th>usecase_finance</th>\n",
       "      <th>upload_nodata</th>\n",
       "      <th>upload_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00095959-e65b-4256-aeba-06464ae106ac</td>\n",
       "      <td>No conversion</td>\n",
       "      <td>0.0</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Diversified Consumer Services</td>\n",
       "      <td>$50M-$100M</td>\n",
       "      <td>251-1K</td>\n",
       "      <td>Utrecht</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000b6e42-2fbd-45c0-9a73-de2cef1ece0d</td>\n",
       "      <td>No conversion</td>\n",
       "      <td>0.0</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Diversified Consumer Services</td>\n",
       "      <td>$1M-$10M</td>\n",
       "      <td>11-50</td>\n",
       "      <td>Mumbai</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000cdc27-c036-4702-9ceb-84ea9f806c3d</td>\n",
       "      <td>No conversion</td>\n",
       "      <td>0.0</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Diversified Consumer Services</td>\n",
       "      <td>$100M-$250M</td>\n",
       "      <td>251-1K</td>\n",
       "      <td>Clinton</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00190170-ab11-4a39-9a14-075e5d3c68ad</td>\n",
       "      <td>No conversion</td>\n",
       "      <td>0.0</td>\n",
       "      <td>FREE_EMAIL</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>001aa956-d9a4-46f0-a528-611936ff34fe</td>\n",
       "      <td>No conversion</td>\n",
       "      <td>0.0</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Diversified Consumer Services</td>\n",
       "      <td>$1B-$10B</td>\n",
       "      <td>1K-5K</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 51 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           workspace_id        product  clv     segment  \\\n",
       "0  00095959-e65b-4256-aeba-06464ae106ac  No conversion  0.0       OTHER   \n",
       "1  000b6e42-2fbd-45c0-9a73-de2cef1ece0d  No conversion  0.0       OTHER   \n",
       "2  000cdc27-c036-4702-9ceb-84ea9f806c3d  No conversion  0.0       OTHER   \n",
       "3  00190170-ab11-4a39-9a14-075e5d3c68ad  No conversion  0.0  FREE_EMAIL   \n",
       "4  001aa956-d9a4-46f0-a528-611936ff34fe  No conversion  0.0       OTHER   \n",
       "\n",
       "   education_flag company_email_flag                       industry  \\\n",
       "0               0                  0  Diversified Consumer Services   \n",
       "1               0                  0  Diversified Consumer Services   \n",
       "2               0                  0  Diversified Consumer Services   \n",
       "3               0                  1                        Unknown   \n",
       "4               1                  0  Diversified Consumer Services   \n",
       "\n",
       "       revenue employees_range     city  ... usecase_management  \\\n",
       "0   $50M-$100M          251-1K  Utrecht  ...                0.0   \n",
       "1     $1M-$10M           11-50   Mumbai  ...                0.0   \n",
       "2  $100M-$250M          251-1K  Clinton  ...                0.0   \n",
       "3      Unknown         Unknown  Unknown  ...                0.0   \n",
       "4     $1B-$10B           1K-5K  Unknown  ...                0.0   \n",
       "\n",
       "  usecase_design  usecase_support  usecase_customer_success usecase_research  \\\n",
       "0            0.0              0.0                       0.0              0.0   \n",
       "1            0.0              0.0                       0.0              0.0   \n",
       "2            0.0              0.0                       0.0              0.0   \n",
       "3            0.0              0.0                       0.0              0.0   \n",
       "4            0.0              0.0                       0.0              0.0   \n",
       "\n",
       "   usecase_operations  usecase_legal  usecase_finance  upload_nodata  \\\n",
       "0                 0.0            0.0              0.0            0.0   \n",
       "1                 0.0            0.0              0.0            0.0   \n",
       "2                 0.0            0.0              0.0            0.0   \n",
       "3                 0.0            0.0              0.0            0.0   \n",
       "4                 0.0            0.0              0.0            0.0   \n",
       "\n",
       "   upload_data  \n",
       "0          0.0  \n",
       "1          0.0  \n",
       "2          0.0  \n",
       "3          0.0  \n",
       "4          0.0  \n",
       "\n",
       "[5 rows x 51 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def fill_missing(data):\n",
    "    # Remove all date columns\n",
    "    data = data.select_dtypes(exclude=['datetime'])\n",
    "\n",
    "    # Remove columns containing 'workspace_id' that are not exactly 'workspace_id'\n",
    "    data = data[[col for col in data.columns if col == 'workspace_id' or 'workspace_id' not in col]]\n",
    "\n",
    "    # Find columns with missing values\n",
    "    missing_columns = data.columns[data.isnull().any()]\n",
    "\n",
    "    # Process each column with missing values\n",
    "    for column in missing_columns:\n",
    "        # If numeric columns, fill in the missing value / NaN as 0\n",
    "        if np.issubdtype(data[column].dtype, np.number):\n",
    "            data[column].fillna(0, inplace=True)\n",
    "        # If categorical columns (strings etc) fill it in as 'Unknown'\n",
    "        else:\n",
    "            data[column].fillna('Unknown', inplace=True)\n",
    "\n",
    "    return data\n",
    "\n",
    "data = fill_missing(data)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f4d317-0898-44a3-b922-05f7f1371e64",
   "metadata": {},
   "source": [
    "# Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ca1c5a-5c59-4a4c-b8b2-89aa6a30f673",
   "metadata": {},
   "source": [
    "### One hot encode\n",
    "\n",
    "\n",
    "One-hot encoding converts categorical variables into a form that can be provided to machine learning algorithms to improve prediction accuracy. It creates binary columns for each category and avoids the misleading ordinal relationships that numeric encoding might imply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f417cd2-fdc5-4ef8-80ca-95461e93e6bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>workspace_id</th>\n",
       "      <th>product</th>\n",
       "      <th>clv</th>\n",
       "      <th>education_flag</th>\n",
       "      <th>ga_signup_flag</th>\n",
       "      <th>fb_signup_flag</th>\n",
       "      <th>project_count</th>\n",
       "      <th>transcription_count</th>\n",
       "      <th>highlight_count</th>\n",
       "      <th>tag_count</th>\n",
       "      <th>...</th>\n",
       "      <th>country_code_VC</th>\n",
       "      <th>country_code_VE</th>\n",
       "      <th>country_code_VI</th>\n",
       "      <th>country_code_VN</th>\n",
       "      <th>country_code_ZA</th>\n",
       "      <th>country_code_ZW</th>\n",
       "      <th>residency_region_Unknown</th>\n",
       "      <th>residency_region_eu-a</th>\n",
       "      <th>residency_region_us-a</th>\n",
       "      <th>residency_region_us-b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00095959-e65b-4256-aeba-06464ae106ac</td>\n",
       "      <td>No conversion</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000b6e42-2fbd-45c0-9a73-de2cef1ece0d</td>\n",
       "      <td>No conversion</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000cdc27-c036-4702-9ceb-84ea9f806c3d</td>\n",
       "      <td>No conversion</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00190170-ab11-4a39-9a14-075e5d3c68ad</td>\n",
       "      <td>No conversion</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>001aa956-d9a4-46f0-a528-611936ff34fe</td>\n",
       "      <td>No conversion</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3539 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           workspace_id        product  clv  education_flag  \\\n",
       "0  00095959-e65b-4256-aeba-06464ae106ac  No conversion  0.0               0   \n",
       "1  000b6e42-2fbd-45c0-9a73-de2cef1ece0d  No conversion  0.0               0   \n",
       "2  000cdc27-c036-4702-9ceb-84ea9f806c3d  No conversion  0.0               0   \n",
       "3  00190170-ab11-4a39-9a14-075e5d3c68ad  No conversion  0.0               0   \n",
       "4  001aa956-d9a4-46f0-a528-611936ff34fe  No conversion  0.0               1   \n",
       "\n",
       "   ga_signup_flag  fb_signup_flag  project_count  transcription_count  \\\n",
       "0               0               0              0                    4   \n",
       "1               0               0              0                    0   \n",
       "2               0               0              0                    0   \n",
       "3               1               0              0                    0   \n",
       "4               1               0              0                    0   \n",
       "\n",
       "   highlight_count  tag_count  ...  country_code_VC  country_code_VE  \\\n",
       "0                0          0  ...              0.0              0.0   \n",
       "1                0          0  ...              0.0              0.0   \n",
       "2                0          0  ...              0.0              0.0   \n",
       "3                0          0  ...              0.0              0.0   \n",
       "4                0          0  ...              0.0              0.0   \n",
       "\n",
       "   country_code_VI  country_code_VN  country_code_ZA  country_code_ZW  \\\n",
       "0              0.0              0.0              0.0              0.0   \n",
       "1              0.0              0.0              0.0              0.0   \n",
       "2              0.0              0.0              0.0              0.0   \n",
       "3              0.0              0.0              0.0              0.0   \n",
       "4              0.0              0.0              0.0              0.0   \n",
       "\n",
       "   residency_region_Unknown  residency_region_eu-a  residency_region_us-a  \\\n",
       "0                       0.0                    0.0                    1.0   \n",
       "1                       0.0                    1.0                    0.0   \n",
       "2                       0.0                    0.0                    1.0   \n",
       "3                       0.0                    0.0                    1.0   \n",
       "4                       0.0                    0.0                    1.0   \n",
       "\n",
       "   residency_region_us-b  \n",
       "0                    0.0  \n",
       "1                    0.0  \n",
       "2                    0.0  \n",
       "3                    0.0  \n",
       "4                    0.0  \n",
       "\n",
       "[5 rows x 3539 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def one_hot_encode(data, exclude):\n",
    "    # Ensure that 'data' is a pandas DataFrame\n",
    "    if not isinstance(data, pd.DataFrame):\n",
    "        raise TypeError(\"Input data must be a pandas DataFrame.\")\n",
    "\n",
    "    # Validate 'exclude' as a list\n",
    "    if not isinstance(exclude, list):\n",
    "        raise TypeError(\"'exclude' must be a list of columns.\")\n",
    "\n",
    "    # Select string and categorical columns to encode, excluding the specified columns\n",
    "    columns_to_encode = data.select_dtypes(include=['object', 'category']).columns\n",
    "    columns_to_encode = [col for col in columns_to_encode if col not in exclude]\n",
    "\n",
    "    # Apply OneHotEncoder\n",
    "    encoder = OneHotEncoder(sparse=False, drop='if_binary')\n",
    "    encoded_data = pd.DataFrame(encoder.fit_transform(data[columns_to_encode]))\n",
    "\n",
    "    # Fix column names after encoding\n",
    "    encoded_data.columns = encoder.get_feature_names_out(columns_to_encode)\n",
    "\n",
    "    # Drop original columns and concatenate encoded data\n",
    "    data = data.drop(columns_to_encode, axis=1)\n",
    "    data = pd.concat([data, encoded_data], axis=1)\n",
    "\n",
    "    return data\n",
    "\n",
    "data = one_hot_encode(data, ['workspace_id', 'product'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fcb42c-3b70-43e3-985c-4931dee4da62",
   "metadata": {},
   "source": [
    "### Best-case performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52a448f9-06ba-4f99-bf86-6049d36fbe64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/patricksweeney/anaconda3/envs/EconML/lib/python3.11/site-packages/sklearn/utils/validation.py:757: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/Users/patricksweeney/anaconda3/envs/EconML/lib/python3.11/site-packages/sklearn/utils/validation.py:595: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/Users/patricksweeney/anaconda3/envs/EconML/lib/python3.11/site-packages/sklearn/utils/validation.py:604: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy of Y-variable: 0.1713146136677787\n",
      "Total Mutual Information (excluding interaction): 1.1715657022045578\n",
      "Proportion of Uncertainty Reduced: 6.838679299575182\n"
     ]
    }
   ],
   "source": [
    "def best_case(data, y_variable, exclude, continuous_y=True):\n",
    "    # Importing necessary libraries\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from scipy.stats import entropy, differential_entropy\n",
    "    from sklearn.feature_selection import mutual_info_regression, mutual_info_classif\n",
    "\n",
    "    # Exclude specified variables and separate X and Y\n",
    "    X = data.drop(columns=[y_variable] + exclude)\n",
    "    Y = data[y_variable]\n",
    "\n",
    "    # Calculate the entropy of Y-variable\n",
    "    if continuous_y:\n",
    "        # Use differential_entropy for continuous Y\n",
    "        y_entropy = differential_entropy(Y)\n",
    "        # Calculate mutual information for continuous Y\n",
    "        mi = mutual_info_regression(X, Y)\n",
    "    else:\n",
    "        # Use entropy for discrete Y\n",
    "        value_counts = Y.value_counts()\n",
    "        y_entropy = entropy(value_counts)\n",
    "        # Calculate mutual information for discrete Y\n",
    "        mi = mutual_info_classif(X, Y)\n",
    "\n",
    "    total_mi = np.sum(mi)\n",
    "\n",
    "    # Proportion of uncertainty reduced\n",
    "    proportion_reduced = total_mi / y_entropy if y_entropy > 0 else 0\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Entropy of Y-variable: {y_entropy}\")\n",
    "    print(f\"Total Mutual Information (excluding interaction): {total_mi}\")\n",
    "    print(f\"Proportion of Uncertainty Reduced: {proportion_reduced}\")\n",
    "\n",
    "# Example usage\n",
    "best_case(data, 'product', ['clv', 'workspace_id'], continuous_y=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cab925-6e42-4015-b601-04cae4d6d5ce",
   "metadata": {},
   "source": [
    "### Remove redundant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1a54cf8-523f-4b2e-931f-7efffe1ed3ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/patricksweeney/anaconda3/envs/EconML/lib/python3.11/site-packages/sklearn/utils/validation.py:757: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if not hasattr(array, \"sparse\") and array.dtypes.apply(is_sparse).any():\n",
      "/Users/patricksweeney/anaconda3/envs/EconML/lib/python3.11/site-packages/sklearn/utils/validation.py:595: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype):\n",
      "/Users/patricksweeney/anaconda3/envs/EconML/lib/python3.11/site-packages/sklearn/utils/validation.py:604: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if is_sparse(pd_dtype) or not is_extension_array_dtype(pd_dtype):\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_selection import SelectPercentile, mutual_info_regression, mutual_info_classif\n",
    "\n",
    "def scikit_prune_features(data, y_variable, exclude, percentile):\n",
    "    # Ensure that 'exclude' is a list\n",
    "    if not isinstance(exclude, list):\n",
    "        raise TypeError(\"'exclude' must be a list of columns.\")\n",
    "\n",
    "    # Separate the features and the target variable\n",
    "    X = data.drop(columns=[y_variable] + exclude)\n",
    "    y = data[y_variable]\n",
    "\n",
    "    # Determine the score function based on the target variable type\n",
    "    if y.dtype == 'float':\n",
    "        score_func = mutual_info_regression\n",
    "    else:\n",
    "        score_func = mutual_info_classif\n",
    "\n",
    "    # Apply SelectPercentile\n",
    "    selector = SelectPercentile(score_func=score_func, percentile=percentile)\n",
    "    X_new = selector.fit_transform(X, y)\n",
    "\n",
    "    # Get the selected feature names\n",
    "    selected_features = X.columns[selector.get_support()]\n",
    "\n",
    "    # Combine selected features with excluded columns and target variable\n",
    "    final_data = pd.concat([data[exclude], data[selected_features], data[[y_variable]]], axis=1)\n",
    "\n",
    "    return final_data\n",
    "\n",
    "# Example usage:\n",
    "data = scikit_prune_features(data, 'product', ['workspace_id', 'clv'], 20)\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee46af7b-ded3-48eb-8eca-307e73a61901",
   "metadata": {},
   "source": [
    "# Predicting $CLV_i$ in first 24-hours (not being used)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a24fc8-79fa-431f-95a5-3b7b0abc8436",
   "metadata": {},
   "source": [
    "Performance here is terrible: 0% $r^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfeeeeb3-1b4a-4f49-bc53-4dc25740ae98",
   "metadata": {},
   "source": [
    "### Regression with gradient boosting\n",
    "\n",
    "\n",
    "Gradient Boosting is a machine learning technique that builds models sequentially, with each new model correcting the errors of the previous ones, optimizing for a loss function. This approach combines weak predictive models, typically decision trees, into a stronger ensemble, offering high accuracy in various tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc402999-a04d-4377-be92-e57d5dd01fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_boosting_regression(data, y_variable, random_state=42):\n",
    "    from sklearn.ensemble import GradientBoostingRegressor\n",
    "    from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "    from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import math\n",
    "    from scipy.interpolate import UnivariateSpline\n",
    "\n",
    "    # Separate the features and target variable\n",
    "    X = data.drop(columns=[y_variable]).select_dtypes(include=np.number)\n",
    "    y = data[y_variable]\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
    "\n",
    "    # 1. Train model\n",
    "    model = GradientBoostingRegressor(random_state=random_state)\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # 2. Test model\n",
    "    cv = KFold(n_splits=10, shuffle=True, random_state=random_state)\n",
    "    cross_val_scores = cross_val_score(model, X, y, cv=cv, scoring='r2')\n",
    "    \n",
    "    # Rounding the scores to two decimal places\n",
    "    rounded_scores = [round(score, 2) for score in cross_val_scores]\n",
    "    print(\"Cross-validation scores (R2):\", rounded_scores)\n",
    "\n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # 3. Performance Metrics\n",
    "    mse = round(mean_squared_error(y_test, y_pred), 2)\n",
    "    r2 = round(r2_score(y_test, y_pred), 2)\n",
    "    mae = round(mean_absolute_error(y_test, y_pred), 2)\n",
    "    print(f\"Mean Squared Error: {mse}\")\n",
    "    print(f\"R2 Score: {r2}\")\n",
    "    print(f\"Mean Absolute Error: {mae}\")\n",
    "\n",
    "    # 4. Plot Predictions vs Actual with adjusted log10 scale\n",
    "    offset = 1e-6  # Small constant to offset zero or negative values\n",
    "    adjusted_y_test = np.log10(y_test + offset)\n",
    "    adjusted_y_pred = np.log10(y_pred + offset)\n",
    "\n",
    "    plt.scatter(adjusted_y_test, adjusted_y_pred)\n",
    "    plt.xlabel(\"Actual Values (log10 scale)\")\n",
    "    plt.ylabel(\"Predicted Values (log10 scale)\")\n",
    "    plt.title(\"Predicted vs Actual Values (log10 scale)\")\n",
    "    \n",
    "    # Line for perfect predictions\n",
    "    min_val = min(adjusted_y_test.min(), adjusted_y_pred.min())\n",
    "    max_val = max(adjusted_y_test.max(), adjusted_y_pred.max())\n",
    "    plt.plot([min_val, max_val], [min_val, max_val], 'k--')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    # 5. Feature Importance - Updated to show only top 10\n",
    "    feature_importance = model.feature_importances_\n",
    "    sorted_idx = np.argsort(feature_importance)[-10:]  # Get the indices of the top 10 features\n",
    "    \n",
    "    plt.barh(X.columns[sorted_idx], feature_importance[sorted_idx])\n",
    "    plt.xlabel(\"Gradient Boosting Feature Importance\")\n",
    "    plt.title(\"Top 10 Features\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return model\n",
    "\n",
    "model = gradient_boosting_regression(data, 'clv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea87b8e-8226-4e89-9dba-2a6e0a575836",
   "metadata": {},
   "source": [
    "# Predicting $Package_i$ in first 24 hours (being used)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb585147-e49b-4c5e-8100-8823471ea305",
   "metadata": {},
   "source": [
    "We need to be minimising our false negative rate, even if we have an overly sensitive classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44eae6ac-2af5-44dc-8429-57bf9b3b720b",
   "metadata": {},
   "source": [
    "### Optional: Make product a binary conversion event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "646c372c-18d5-41e0-8f1b-a6a4370eaa2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>workspace_id</th>\n",
       "      <th>product</th>\n",
       "      <th>clv</th>\n",
       "      <th>segment</th>\n",
       "      <th>education_flag</th>\n",
       "      <th>company_email_flag</th>\n",
       "      <th>industry</th>\n",
       "      <th>revenue</th>\n",
       "      <th>employees_range</th>\n",
       "      <th>city</th>\n",
       "      <th>...</th>\n",
       "      <th>transcription_count</th>\n",
       "      <th>highlight_count</th>\n",
       "      <th>tag_count</th>\n",
       "      <th>insight_count</th>\n",
       "      <th>reel_created_count</th>\n",
       "      <th>invite_count</th>\n",
       "      <th>shared_object_note_count</th>\n",
       "      <th>shared_object_insight_count</th>\n",
       "      <th>note_viewed_user_count</th>\n",
       "      <th>tag_viewed_user_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00095959-e65b-4256-aeba-06464ae106ac</td>\n",
       "      <td>No conversion</td>\n",
       "      <td>0.0</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Diversified Consumer Services</td>\n",
       "      <td>$50M-$100M</td>\n",
       "      <td>251-1K</td>\n",
       "      <td>Utrecht</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00786b99-40f5-4703-a772-3026df9827ff</td>\n",
       "      <td>No conversion</td>\n",
       "      <td>0.0</td>\n",
       "      <td>FREE_EMAIL</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Software &amp; Services</td>\n",
       "      <td>$10B+</td>\n",
       "      <td>100K+</td>\n",
       "      <td>Mountain View</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00ddc9a5-85c6-44d9-9968-c37cdad31fcc</td>\n",
       "      <td>No conversion</td>\n",
       "      <td>0.0</td>\n",
       "      <td>FREE_EMAIL</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0160b311-e4f8-4bbd-a06f-e2b4c80d40a1</td>\n",
       "      <td>No conversion</td>\n",
       "      <td>0.0</td>\n",
       "      <td>FREE_EMAIL</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0172345b-7159-4c99-88d9-3e4fa521f14d</td>\n",
       "      <td>No conversion</td>\n",
       "      <td>0.0</td>\n",
       "      <td>FREE_EMAIL</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           workspace_id        product  clv     segment  \\\n",
       "0  00095959-e65b-4256-aeba-06464ae106ac  No conversion  0.0       OTHER   \n",
       "1  00786b99-40f5-4703-a772-3026df9827ff  No conversion  0.0  FREE_EMAIL   \n",
       "2  00ddc9a5-85c6-44d9-9968-c37cdad31fcc  No conversion  0.0  FREE_EMAIL   \n",
       "3  0160b311-e4f8-4bbd-a06f-e2b4c80d40a1  No conversion  0.0  FREE_EMAIL   \n",
       "4  0172345b-7159-4c99-88d9-3e4fa521f14d  No conversion  0.0  FREE_EMAIL   \n",
       "\n",
       "   education_flag company_email_flag                       industry  \\\n",
       "0               0                  0  Diversified Consumer Services   \n",
       "1               0                  1            Software & Services   \n",
       "2               0                  1                        Unknown   \n",
       "3               0                  1                        Unknown   \n",
       "4               0                  1                        Unknown   \n",
       "\n",
       "      revenue employees_range           city  ... transcription_count  \\\n",
       "0  $50M-$100M          251-1K        Utrecht  ...                   4   \n",
       "1       $10B+           100K+  Mountain View  ...                   0   \n",
       "2     Unknown         Unknown        Unknown  ...                   0   \n",
       "3     Unknown         Unknown        Unknown  ...                   0   \n",
       "4     Unknown         Unknown        Unknown  ...                   0   \n",
       "\n",
       "  highlight_count  tag_count  insight_count reel_created_count  invite_count  \\\n",
       "0               0          0              0                  0             0   \n",
       "1               0          0              0                  0             0   \n",
       "2               0          0              0                  0             0   \n",
       "3               0          0              0                  0             0   \n",
       "4               0          0              0                  0             0   \n",
       "\n",
       "   shared_object_note_count  shared_object_insight_count  \\\n",
       "0                         0                            0   \n",
       "1                         0                            0   \n",
       "2                         0                            0   \n",
       "3                         0                            0   \n",
       "4                         0                            0   \n",
       "\n",
       "   note_viewed_user_count  tag_viewed_user_count  \n",
       "0                       0                      0  \n",
       "1                       0                      0  \n",
       "2                       0                      0  \n",
       "3                       1                      0  \n",
       "4                       0                      0  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def make_binary_conversion(data, y_variable):\n",
    "    # Check if y_variable exists in the dataframe\n",
    "    if y_variable not in data.columns:\n",
    "        raise ValueError(f\"{y_variable} is not a column in the provided dataframe.\")\n",
    "\n",
    "    # Convert the y_variable to binary\n",
    "    data[y_variable] = data[y_variable].apply(lambda x: 'No conversion' if x == 'No conversion' else 'Conversion')\n",
    "\n",
    "    return data\n",
    "\n",
    "data = make_binary_conversion(data, 'product')\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e862bfa1-56c7-4ea9-ae8e-bbc32ec93b6b",
   "metadata": {},
   "source": [
    "### Classification with gradient boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18e7a6a4-ad28-45cc-aab7-ed608a5c9a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_boosting(data, y_variable, exclude):\n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "    from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "    from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc, precision_recall_curve, cohen_kappa_score\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    random_state = 2\n",
    "    \n",
    "    # Ensure 'exclude' is a list\n",
    "    if not isinstance(exclude, list):\n",
    "        raise TypeError(\"'exclude' must be a list of columns.\")\n",
    "\n",
    "    # Separate features and target variable\n",
    "    X = data.drop(columns=[y_variable] + exclude).select_dtypes(include=np.number)\n",
    "    y = data[y_variable]\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
    "\n",
    "    # Train model\n",
    "    model = GradientBoostingClassifier()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Test model\n",
    "    cv = StratifiedKFold(n_splits=10)\n",
    "    cross_val_scores = cross_val_score(model, X, y, cv=cv, scoring='f1_macro')\n",
    "    print(\"Cross-validation scores:\", cross_val_scores)\n",
    "\n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)\n",
    "\n",
    "    # 4. Classification report\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    # Confusion matrix\n",
    "    print(\"Confusion matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "    # Compute ROC, AUC, Precision-Recall for each class\n",
    "    classes = np.unique(y)\n",
    "    for i, cls in enumerate(classes):\n",
    "        fpr, tpr, _ = roc_curve((y_test == cls).astype(int), y_proba[:, i])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        precision, recall, _ = precision_recall_curve((y_test == cls).astype(int), y_proba[:, i])\n",
    "        \n",
    "        # ROC Curve\n",
    "        plt.figure()\n",
    "        plt.plot(fpr, tpr, label='Class %s AUC = %0.2f' % (cls, roc_auc))\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('ROC for class %s' % cls)\n",
    "        plt.legend(loc=\"lower right\")\n",
    "        plt.show()\n",
    "\n",
    "        # Precision-Recall Curve\n",
    "        plt.figure()\n",
    "        plt.plot(recall, precision)\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.title('Precision-Recall Curve for class %s' % cls)\n",
    "        plt.show()\n",
    "    \n",
    "\n",
    "    # # Compute and plot Lift Chart\n",
    "    # df_lift = pd.DataFrame({'y_test': y_test, 'y_proba': y_proba})\n",
    "    # df_lift = df_lift.sort_values(by='y_proba', ascending=False)\n",
    "    # df_lift['decile'] = pd.qcut(df_lift['y_proba'], 10, labels=False)\n",
    "    # df_lift['num_positive'] = df_lift['y_test'].cumsum()\n",
    "    # df_lift['total'] = df_lift.index + 1\n",
    "    # df_lift['lift'] = df_lift['num_positive'] / df_lift['total']\n",
    "\n",
    "    \n",
    "    # Cross-validation score\n",
    "    print(\"Average cross-validation score:\", np.mean(cross_val_scores))\n",
    "\n",
    "    # Cohen's Kappa\n",
    "    kappa = cohen_kappa_score(y_test, y_pred)\n",
    "    print(\"Cohen's Kappa:\", kappa)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b82d62f-ce86-4791-84f8-52355aee472e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gradient_boosting(data, 'product', ['clv'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab9fad1-a567-479f-9460-79077b57bfc7",
   "metadata": {},
   "source": [
    "# Get value-weightings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb18937-4bb2-4d4e-9bfc-1c2f5de68865",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(data, model, y_variable, exclude):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    # Filter data where the target variable equals 0\n",
    "    target_data = data\n",
    "\n",
    "    # Exclude specified variables, separate the features, and retain the index\n",
    "    X_target = target_data.drop(columns=[y_variable] + exclude).select_dtypes(include=np.number)\n",
    "\n",
    "    # Make predictions\n",
    "    predictions = model.predict(X_target)\n",
    "    probabilities = model.predict_proba(X_target)[:, 1]\n",
    "\n",
    "    # Append predictions and probabilities to the original data\n",
    "    data.loc[target_data.index, 'Prediction'] = predictions\n",
    "    data.loc[target_data.index, 'Probability'] = probabilities\n",
    "\n",
    "    return data\n",
    "\n",
    "# Example usage\n",
    "predictions = get_predictions(data, model, 'product', ['clv'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3d692d-926f-44c2-885c-055d694e8db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weightings(data, prediction_column, clv_column):\n",
    "    import pandas as pd\n",
    "\n",
    "    # Calculate the average CLV for each prediction category\n",
    "    average_clvs = data.groupby(prediction_column)[clv_column].mean()\n",
    "\n",
    "    # Create a new column 'weighting' with the average CLV for each prediction\n",
    "    data['weighting'] = data[prediction_column].map(average_clvs)\n",
    "\n",
    "    # Select only the necessary columns\n",
    "    data = data[['workspace_id', prediction_column, 'weighting']]\n",
    "\n",
    "    return data\n",
    "\n",
    "weightings = get_weightings(data, 'Prediction', 'clv')\n",
    "weightings.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6504c008-90b8-4df3-abf8-1303dcec7ad4",
   "metadata": {},
   "source": [
    "# Save predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae549e0-617a-4706-9ecc-fe7fb9ad5ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_predictions(predictions, filename):\n",
    "    import pandas as pd\n",
    "\n",
    "    # Ensure the filename ends with '.xlsx'\n",
    "    if not filename.endswith('.xlsx'):\n",
    "        filename += '.xlsx'\n",
    "\n",
    "    # Save to Excel\n",
    "    predictions.to_excel(filename, index=False)\n",
    "\n",
    "save_predictions(weightings, 'predictions_vbb.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2441cca6-7147-4696-8828-16e02b150862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EconML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
